{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "a = 0.3\n",
    "b = 0.5\n",
    "\n",
    "def lambda1(beta1, beta2):\n",
    "    nom = (b-a)*(beta1-beta2)\n",
    "    denom = (b-a-1)*beta2-(b-a)*beta1\n",
    "    \n",
    "    return nom/denom\n",
    "\n",
    "def lambda2(beta1, beta2):\n",
    "    nom = (b-a-1)*(beta1-beta2)\n",
    "    denom = (b-a-1)*beta2-(b-a)*beta1\n",
    "    \n",
    "    return nom/denom\n",
    "\n",
    "def effbeta(beta1, beta2):\n",
    "    lam1 = lambda1(beta1, beta2)\n",
    "    lam2 = lambda2(beta1, beta2)\n",
    "    \n",
    "    beta = beta1*(lam1+1)*(1+a-b)+beta2*(b-a)*(lam2+1)\n",
    "    return beta"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Create a meshgrid for the range of beta1 and beta2 values\n",
    "beta1_range = np.linspace(0.05, 400, 100)\n",
    "beta2_range = np.linspace(0.05, 400, 100)\n",
    "beta1, beta2 = np.meshgrid(beta1_range, beta2_range)\n",
    "\n",
    "# Compute effbeta for each combination of beta1 and beta2\n",
    "eff_beta_values = effbeta(beta1, beta2)\n",
    "\n",
    "# Create a heat plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(eff_beta_values, extent=(0.05, 400, 0.05, 400), origin='lower', aspect='auto')\n",
    "plt.colorbar(label='effbeta')\n",
    "plt.xlabel('beta1')\n",
    "plt.ylabel('beta2')\n",
    "plt.title('Heat Plot of effbeta')\n",
    "plt.show()\n",
    "\n",
    "# Create a surface plot\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "surf = ax.plot_surface(beta1, beta2, eff_beta_values, cmap='viridis')\n",
    "fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n",
    "ax.set_xlabel('beta1')\n",
    "ax.set_ylabel('beta2')\n",
    "ax.set_zlabel('effbeta')\n",
    "ax.set_title('Surface Plot of effbeta')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "\n",
    "class ClassicalSurrogate(nn.Module):\n",
    "    def __init__(self, z_function, omega_spectrum):\n",
    "        super(ClassicalSurrogate, self).__init__()\n",
    "        self.omegas = omega_spectrum\n",
    "        \n",
    "        self.alpha = nn.Parameter(torch.empty(self.omegas.size(0), 1, dtype=torch.float64))\n",
    "        self.beta = nn.Parameter(torch.empty(self.omegas.size(0), 1, dtype=torch.float64))\n",
    "        nn.init.normal_(self.alpha)\n",
    "        nn.init.normal_(self.beta)\n",
    "        self.z_function = z_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_x = self.z_function(x)\n",
    "        \n",
    "        # Compute dot product\n",
    "        dot_products = torch.mm(z_x, self.omegas.t())\n",
    "        \n",
    "        # Compute cosine and sine values\n",
    "        cos_values = torch.cos(dot_products)\n",
    "        sin_values = torch.sin(dot_products)\n",
    "        \n",
    "        # Weighted sum of cosine and sine values for each sample\n",
    "        outputs = torch.sum(self.alpha.squeeze() * cos_values + self.beta.squeeze() * sin_values, dim=1)\n",
    "        return outputs.unsqueeze(1)\n",
    "\n",
    "# Example usage:\n",
    "def id_z(x):\n",
    "    # For demonstration, we're returning x. Replace this with any required transformation.\n",
    "    return x\n",
    "\n",
    "# Assuming z(x) is a 3-dimensional vector, and each component can take values [1.0, 2.0] or [2.0, 3.0] or [1.0, 3.0]\n",
    "L = 4\n",
    "omega_spectrum = list(range(-L, L+1))\n",
    "#all_combinations = list(itertools.product(*omega_spectrum))\n",
    "all_combinations = omega_spectrum\n",
    "all_combinations = torch.tensor(all_combinations, dtype=torch.float64).reshape(L*2+1, 1)\n",
    "model = ClassicalSurrogate(id_z, all_combinations)\n",
    "\n",
    "# Example forward pass:\n",
    "x = torch.tensor([[1.0], [2.0]], dtype=torch.float64)\n",
    "output = model(x)\n",
    "print(output)\n",
    "print(output.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def square_loss(targets, predictions):\n",
    "    loss = 0\n",
    "    for t, p in zip(targets, predictions):\n",
    "        loss += (t - p) ** 2\n",
    "    loss = loss / len(targets)\n",
    "    return 0.5*loss\n",
    "\n",
    "\n",
    "degree = 1  # degree of the target function\n",
    "scaling = 1  # scaling of the data\n",
    "coeffs = [0.15 + 0.15j]*degree  # coefficients of non-zero frequencies\n",
    "coeff0 = 0.1  # coefficient of zero frequency\n",
    "\n",
    "def target_function(x):\n",
    "    \"\"\"Generate a truncated Fourier series, where the data gets re-scaled.\"\"\"\n",
    "    res = coeff0\n",
    "    for idx, coeff in enumerate(coeffs):\n",
    "        exponent = np.complex128(scaling * (idx+1) * x * 1j)\n",
    "        conj_coeff = np.conjugate(coeff)\n",
    "        res += coeff * np.exp(exponent) + conj_coeff * np.exp(-exponent)\n",
    "    return np.real(res)\n",
    "\n",
    "x = np.linspace(-6, 6, 70, requires_grad=False)\n",
    "target_y = np.array([target_function(x_) for x_ in x], requires_grad=False)\n",
    "\n",
    "plt.plot(x, target_y, c='black')\n",
    "plt.scatter(x, target_y, facecolor='white', edgecolor='black')\n",
    "plt.ylim(-1, 1)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "scaling = 1\n",
    "\n",
    "dev = qml.device('default.qubit', wires=1)\n",
    "\n",
    "def S(x):\n",
    "    \"\"\"Data-encoding circuit block.\"\"\"\n",
    "    qml.RX(scaling * x, wires=0)\n",
    "\n",
    "def W(theta):\n",
    "    \"\"\"Trainable circuit block.\"\"\"\n",
    "    qml.Rot(theta[0], theta[1], theta[2], wires=0)\n",
    "\n",
    "\n",
    "@qml.qnode(dev, interface=\"autograd\")\n",
    "def serial_quantum_model(weights, x):\n",
    "\n",
    "    for theta in weights[:-1]:\n",
    "        W(theta)\n",
    "        S(x)\n",
    "\n",
    "    # (L+1)'th unitary\n",
    "    W(weights[-1])\n",
    "\n",
    "    return qml.expval(qml.PauliZ(wires=0))\n",
    "\n",
    "r = 1 # number of times the encoding gets repeated (here equal to the number of layers)\n",
    "weights = 2 * np.pi * np.random.random(size=(r+1, 3), requires_grad=True) # some random initial weights\n",
    "\n",
    "x = np.linspace(-6, 6, 70, requires_grad=False)\n",
    "random_quantum_model_y = [serial_quantum_model(weights, x_) for x_ in x]\n",
    "\n",
    "plt.plot(x, random_quantum_model_y, c='blue')\n",
    "#plt.ylim(-1,1)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "drawer = qml.draw_mpl(serial_quantum_model, show_all_wires=True, expansion_strategy=\"device\")\n",
    "x = 0.0\n",
    "print(drawer(weights, x))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def cost(weights, x, y):\n",
    "    predictions = [serial_quantum_model(weights, x_) for x_ in x]\n",
    "    return square_loss(y, predictions)\n",
    "\n",
    "max_steps = 60\n",
    "opt = qml.AdamOptimizer(0.3)\n",
    "batch_size = 25\n",
    "cst = [cost(weights, x, target_y)]  # initial cost\n",
    "\n",
    "for step in range(max_steps):\n",
    "\n",
    "    # Select batch of data\n",
    "    batch_index = np.random.randint(0, len(x), (batch_size,))\n",
    "    x_batch = x[batch_index]\n",
    "    y_batch = target_y[batch_index]\n",
    "\n",
    "    # Update the weights by one optimizer step\n",
    "    weights, _, _ = opt.step(cost, weights, x_batch, y_batch)\n",
    "\n",
    "    # Save, and possibly print, the current cost\n",
    "    c = cost(weights, x, target_y)\n",
    "    cst.append(c)\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(\"Cost at step {0:3}: {1}\".format(step + 1, c))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "predictions = [serial_quantum_model(weights, x_) for x_ in x]\n",
    "\n",
    "plt.plot(x, target_y, c='black')\n",
    "plt.scatter(x, target_y, facecolor='white', edgecolor='black')\n",
    "plt.plot(x, predictions, c='blue')\n",
    "plt.ylim(-1,1)\n",
    "plt.show();"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "from qulearn.qlayer import AltRotCXLayer, HamiltonianLayer\n",
    "\n",
    "num_qubits = 3\n",
    "n_layers = 2\n",
    "wires = range(0, num_qubits)\n",
    "dev = qml.device(\"default.qubit\", wires=wires, shots=None)\n",
    "layer = AltRotCXLayer(wires, n_layers=n_layers, dtype=torch.float64)\n",
    "obs = [qml.PauliZ(0)]\n",
    "model = HamiltonianLayer(layer, observables=obs, qdevice=dev, dtype=torch.float64, interface=\"torch\", diff_method=\"backprop\")\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "y = torch.tensor([-0.25], dtype=torch.float64)\n",
    "drawer = qml.draw(model.qnode, show_all_wires=True, expansion_strategy=\"device\")\n",
    "print(drawer())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "Z = np.array([[1, 0], [0, -1]])\n",
    "I = np.eye(2)\n",
    "matrices = [np.kron(np.kron(a, b), c) for a in [I, Z] for b in [I, Z] for c in [I, Z]]\n",
    "coefficients = np.random.rand(len(matrices))\n",
    "\n",
    "M = sum(matrices[i]*coefficients[i] for i in range(2))\n",
    "eigenvalues = np.linalg.eigvals(M)\n",
    "print(eigenvalues)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from torch.optim import Adam\n",
    "opt = Adam(model.parameters(), lr=0.1)\n",
    "opt.zero_grad()\n",
    "pred = model()\n",
    "loss = loss_fn(pred, y)\n",
    "loss.backward()\n",
    "opt.step()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "drawer = qml.draw(model.qnode, show_all_wires=True, expansion_strategy=\"device\")\n",
    "print(drawer())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def scalar2vector(x, L):\n",
    "    # Check if input is within the expected range\n",
    "    if not -1 <= x < 1:\n",
    "        raise ValueError(\"x must be in the range [-1, 1)\")\n",
    "    if L <= 0:\n",
    "        raise ValueError(\"L must be a positive integer\")\n",
    "    \n",
    "    # Initialize binary and remainder variables\n",
    "    binary = [0] * L\n",
    "    reference = 0.0\n",
    "\n",
    "    # Create binary variables\n",
    "    for i in range(L):\n",
    "        if x >= reference:\n",
    "            binary[i] = 1\n",
    "            if i < L-1:\n",
    "                reference += 0.5**(i+1)\n",
    "        else:\n",
    "            if i < L-1:\n",
    "                reference -= 0.5**(i+1)\n",
    "\n",
    "    # Adjust remainder\n",
    "    remainder = x - reference\n",
    "    remainder *= 2 ** (L - 1)\n",
    "\n",
    "    return (binary, remainder)\n",
    "\n",
    "def vector2scalar(binary, remainder):\n",
    "    L = len(binary)\n",
    "    sum = -1.0\n",
    "    for j, ij in enumerate(binary):\n",
    "        sum += 2.0**(-j)*ij\n",
    "        \n",
    "    sum += remainder*2.0**(1-L)\n",
    "    \n",
    "    return sum\n",
    "\n",
    "x = torch.tensor([0.8])\n",
    "L = 3\n",
    "vec = scalar2vector(x, L)\n",
    "orig = vector2scalar(*vec)\n",
    "print(x)\n",
    "print(vec)\n",
    "print(orig)\n",
    "\n",
    "if x < 0:\n",
    "    print(\"less than 0\")\n",
    "if x > 0:\n",
    "    print(\"larger than 0\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from qulearn.qlayer import CircuitLayer\n",
    "import pennylane as qml\n",
    "\n",
    "class QTT1DEncoding(CircuitLayer):\n",
    "    \n",
    "    def __init__(self, wires, level):\n",
    "        super().__init__(wires)\n",
    "        self.level = level\n",
    "        \n",
    "    def circuit(self, x):\n",
    "        binary, y = scalar2vector(x, self.level)\n",
    "        \n",
    "        for index, b in enumerate(binary):\n",
    "            if b == 1:\n",
    "                qml.PauliX(self.wires[index])\n",
    "                \n",
    "        qml.RZ(0.0, self.wires[-1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from qulearn.qlayer import CircuitLayer, AltRotCXLayer\n",
    "import pennylane as qml\n",
    "from pennylane import IQPEmbedding\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Hadamards(CircuitLayer):\n",
    "    \n",
    "    def __init__(self, wires):\n",
    "        super().__init__(wires)\n",
    "        \n",
    "    def circuit(self, x):\n",
    "        for wire in self.wires:\n",
    "            qml.Hadamard(wire)\n",
    "\n",
    "class Exp1DEncoding(CircuitLayer):\n",
    "    \n",
    "    def __init__(self, wires):\n",
    "        super().__init__(wires)\n",
    "        \n",
    "    def circuit(self, x):\n",
    "        lrelu = torch.nn.LeakyReLU(0.01)\n",
    "        for j, wire in enumerate(self.wires):\n",
    "            x_ = torch.asin(lrelu(x))\n",
    "            x_ = lrelu(x)\n",
    "            x_ = 2**j*x\n",
    "            qml.RZ(x_.item(), wire)\n",
    "            \n",
    "class ParallelIQPEncoding(CircuitLayer):\n",
    "    \n",
    "    def __init__(self, wires, num_features, omega = 1.0):\n",
    "        super().__init__(wires)\n",
    "        self.num_features = num_features\n",
    "        self.omega = omega\n",
    "        \n",
    "        assert self.num_wires >= self.num_features\n",
    "        assert self.num_wires % self.num_features == 0\n",
    "        \n",
    "    def circuit(self, x):\n",
    "        num_features = x.shape[-1]\n",
    "        assert num_features == self.num_features\n",
    "        \n",
    "        freq = 0\n",
    "        for i in range(0, len(self.wires), num_features):\n",
    "            x_ = 2**(freq*self.omega)*x\n",
    "            qml.IQPEmbedding(x_, wires=self.wires[i: i+num_features])\n",
    "            freq += 1\n",
    "            \n",
    "class ParallelEntangledIQPEncoding(CircuitLayer):\n",
    "    \n",
    "    def __init__(self, wires, num_features, omega = 1.0):\n",
    "        super().__init__(wires)\n",
    "        self.num_features = num_features\n",
    "        self.omega = omega\n",
    "        \n",
    "        assert self.num_wires >= self.num_features\n",
    "        assert self.num_wires % self.num_features == 0\n",
    "        \n",
    "    def circuit(self, x):\n",
    "        num_features = x.shape[-1]\n",
    "        assert num_features == self.num_features\n",
    "        \n",
    "        num_repeats = int(self.num_wires/num_features)\n",
    "        \n",
    "        x_large = []\n",
    "        for j in range(0, num_repeats):\n",
    "            x_ = 2**(j*self.omega)*x\n",
    "            x_large.append(x_)\n",
    "            \n",
    "        x_final = torch.cat(x_large)\n",
    "        qml.IQPEmbedding(x_final, wires=self.wires)\n",
    "        \n",
    "\n",
    "class QuantumKernel(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed, X_train):\n",
    "        super().__init__()\n",
    "        self.embed = embed\n",
    "        self.X_train = X_train\n",
    "        self.num_samples = X_train.shape[0]\n",
    "        self.alpha = nn.Parameter(\n",
    "            torch.empty(\n",
    "                self.num_samples,\n",
    "                device=self.X_train.device,\n",
    "                dtype=self.X_train.dtype,\n",
    "            )\n",
    "        )\n",
    "        nn.init.normal_(self.alpha)\n",
    "        \n",
    "        self.qdevice = qml.device(wires=self.embed.wires, name=\"default.qubit\", shots=None)\n",
    "        self.qnode = self.set_qnode()\n",
    "        \n",
    "    \n",
    "    def kernel_circ(self, x, x_):\n",
    "        self.embed(x)\n",
    "        qml.adjoint(self.embed)(x_)\n",
    "        state = torch.zeros(self.embed.num_wires, dtype=torch.int32, device=x.device)\n",
    "        projector = qml.Projector(state, self.embed.wires)\n",
    "        return qml.expval(projector)\n",
    "    \n",
    "    def kernel_matrix(self, x, x_):\n",
    "        # assert shape has length 2\n",
    "        \n",
    "        K = torch.empty((x.shape[0], x_.shape[0]), dtype=x.dtype, device=x.device)\n",
    "        for i, xi in enumerate(x):\n",
    "            for j, xj in enumerate(x_):\n",
    "                Kij = self.qnode(xi, xj)\n",
    "                K[i, j] = Kij\n",
    "        return K\n",
    "    \n",
    "    def kernel_ridge_regression(self, labels, lambda_reg) -> None:\n",
    "        # sets optimal alpha (closed form solution)\n",
    "        K = self.kernel_matrix(self.X_train, self.X_train)\n",
    "        I = torch.eye(self.num_samples, dtype=labels.dtype, device=labels.device)\n",
    "        M = K+lambda_reg*I\n",
    "        self.alpha = nn.Parameter(torch.linalg.solve(M, labels))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        K = self.kernel_matrix(x, self.X_train)\n",
    "        out = torch.matmul(K, self.alpha)\n",
    "        return out\n",
    "    \n",
    "    def set_qnode(self):\n",
    "        circuit = self.kernel_circ\n",
    "        qnode = qml.QNode(\n",
    "            circuit,\n",
    "            self.qdevice,\n",
    "            interface=\"torch\",\n",
    "            diff_method=\"backprop\"\n",
    "        )\n",
    "        self.qnode = qnode\n",
    "        return self.qnode"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from enum import Enum\n",
    "class Test(Enum):\n",
    "    one = 1\n",
    "    two = 2\n",
    "    \n",
    "print(Test.one.name)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "x1 = torch.tensor([[0.1]])\n",
    "x2 = torch.tensor([[0.2]])\n",
    "x3 = torch.tensor([[0.3]])\n",
    "x = torch.cat((x1, x2, x3), dim=1)\n",
    "print(x)\n",
    "print(x.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pennylane as qml\n",
    "from qulearn.qlayer import ParallelEntangledIQPEncoding, ParallelIQPEncoding, MeasurementLayer, MeasurementType, IQPERYCZLayer, RYCZLayer\n",
    "from qulearn.qkernel import QKernel\n",
    "from qulearn.trainer import RidgeRegression\n",
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "num_features = 2\n",
    "num_feature_repeat = 2\n",
    "wires = num_features*num_feature_repeat\n",
    "embed = ParallelEntangledIQPEncoding(wires, num_features=num_features, omega=1.0, n_repeat=2)\n",
    "var = RYCZLayer(wires=wires, n_layers=1)\n",
    "#embed = IQPERYCZLayer(wires=wires, num_uploads=1, num_varlayers=0, num_repeat=2, omega=1.0)\n",
    "#embed = ParallelIQPEncoding(wires, num_features=1, omega=1.0, n_repeat=2)\n",
    "obs = qml.PauliZ(0)\n",
    "num_samples = 10\n",
    "X_train = torch.randn((num_samples, num_features))\n",
    "labels = torch.randn((num_samples, 1))\n",
    "model = QKernel(embed, X_train)\n",
    "#model = MeasurementLayer(var, embed, measurement_type=MeasurementType.Expectation, observable=obs)\n",
    "drawer = qml.draw(model.qnode, show_all_wires=True, expansion_strategy=\"device\")\n",
    "x1 = torch.tensor([0.1, 0.3])\n",
    "x2 = torch.tensor([0.2, 0.3])\n",
    "print(drawer(x1, x2))\n",
    "\n",
    "# #test = model.qnode(x1, x2)\n",
    "# test = model.kernel_matrix(X_train, X_train)\n",
    "# print(test.shape)\n",
    "# print(test)\n",
    "\n",
    "# predicted = model(X_train)\n",
    "# fn = MSELoss()\n",
    "# loss_before = fn(predicted, labels)\n",
    "\n",
    "# lambda_reg = 1.0\n",
    "# metrics = {\"mse_loss\": fn}\n",
    "# logger = logging.getLogger(__name__)\n",
    "# logger.setLevel(level=logging.INFO)\n",
    "# trainer = RidgeRegression(lambda_reg, metrics, logger)\n",
    "# dataset = TensorDataset(X_train, labels)\n",
    "# train_loader = DataLoader(dataset, batch_size=num_samples)\n",
    "# valid_loader = DataLoader(dataset, batch_size=num_samples)\n",
    "# trainer.train(model, train_loader, valid_loader)\n",
    "\n",
    "# predicted = model(X_train)\n",
    "# loss_after = fn(predicted, labels)\n",
    "\n",
    "# print(f\"Loss before: {loss_before} | Loss after: {loss_after}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(model.alpha)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "X_train = torch.randn((num_samples, num_features))\n",
    "model.X_train = X_train\n",
    "print(model.alpha)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "for p in model.named_parameters():\n",
    "    print(p)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from qulearn.qlayer import HamiltonianLayer, AltRotCXLayer, IQPEmbeddingLayer\n",
    "import torch\n",
    "\n",
    "observables = [qml.PauliZ(0), qml.Identity(0)]#, qml.PauliZ(1), qml.PauliZ(2), qml.PauliZ(3)]\n",
    "wires = 1*3\n",
    "embed = ParallelEntangledIQPEncoding(wires, num_features=1)\n",
    "#embed = ParallelIQPEncoding(wires, num_features=1)\n",
    "varlayer1 = AltRotCXLayer(wires=3, n_layers=2)\n",
    "varlayer2 = AltRotCXLayer(wires=3, n_layers=2)\n",
    "model = HamiltonianLayer(varlayer1, embed, varlayer2, observables=observables)\n",
    "drawer = qml.draw(model.qnode, show_all_wires=True, expansion_strategy=\"device\")\n",
    "\n",
    "x = torch.rand((1, 2))*2.0-1.0\n",
    "x = torch.tensor([[-0.74]])\n",
    "y = model(x)\n",
    "print(y)\n",
    "x_ = x[0]\n",
    "print(drawer(x_))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from qulearn.qlayer import HamiltonianLayer, AltRotCXLayer, IQPEmbeddingLayer\n",
    "import torch\n",
    "\n",
    "observables = [qml.PauliZ(0)]#, qml.PauliZ(1), qml.PauliZ(2), qml.PauliZ(3)]\n",
    "#embed = QTT1DEncoding(wires=4, level=3)\n",
    "#embed = IQPEmbeddingLayer(wires=1, n_repeat=1)\n",
    "hads = Hadamards(wires=4)\n",
    "embed = Exp1DEncoding(wires=4)\n",
    "varlayer1 = AltRotCXLayer(wires=4, n_layers=2)\n",
    "varlayer2 = AltRotCXLayer(wires=4, n_layers=2)\n",
    "model = HamiltonianLayer(hads, varlayer1, embed, varlayer2, observables=observables)\n",
    "drawer = qml.draw(model.qnode, show_all_wires=True, expansion_strategy=\"device\")\n",
    "\n",
    "x = torch.rand((1, 1))*2.0-1.0\n",
    "x = torch.tensor([[-0.74]])\n",
    "y = model(x)\n",
    "print(y)\n",
    "print(drawer(x))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def fourier_series(x, n, c):\n",
    "    \"\"\"\n",
    "    Compute the truncated Fourier series evaluated at an input x.\n",
    "    \"\"\"\n",
    "    k = torch.arange(-n, n+1)\n",
    "    terms = c[k+n]*torch.exp(1j*k*x)\n",
    "    y = torch.real(torch.sum(terms, dim=1))\n",
    "    y = y/torch.max(torch.abs(y))\n",
    "    return y\n",
    "\n",
    "def generate_dataset(num_samples, n, c):\n",
    "    \"\"\"\n",
    "    Generate a dataset of random samples x with corresponding y values evaluated \n",
    "    at x using a truncated Fourier series.\n",
    "    \"\"\"\n",
    "    x = torch.rand(num_samples, 1, dtype=torch.float64)*2.0-1.0\n",
    "    y = fourier_series(x, n, c)\n",
    "    return x, y\n",
    "\n",
    "def get_data_loader(x, y, batch_size=5):\n",
    "    \"\"\"\n",
    "    Convert the dataset into a PyTorch DataLoader.\n",
    "    \"\"\"\n",
    "    dataset = TensorDataset(x.unsqueeze(-1), y.unsqueeze(-1))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "n = 10  # Number of terms in the Fourier series\n",
    "c_positive = torch.rand(n) + 1j*torch.rand(n)  # Random coefficients for positive k\n",
    "c_negative = torch.conj(c_positive)  # Coefficients for negative k are the conjugate of those for positive k\n",
    "c_positive = c_positive.flip(dims=[0])\n",
    "c_zero = torch.rand(1)  # Coefficient for k=0 is just a real number\n",
    "c = torch.cat((c_negative, c_zero, c_positive))\n",
    "\n",
    "# Generate a dataset and data loader\n",
    "x, y = generate_dataset(25, n, c)\n",
    "y = torch.where(x <= 0, torch.tensor(0, dtype=torch.float64), torch.tensor(1, dtype=torch.float64))\n",
    "dataloader = get_data_loader(x, y)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pennylane as qml\n",
    "from qulearn.qlayer import MeasurementLayer, MeasurementType, IQPEAltRotCXLayer, HamiltonianLayer\n",
    "num_features = 3\n",
    "interface='torch'\n",
    "diff_method='backprop'\n",
    "qdev = qml.device(\"default.qubit\", wires=num_features, shots=None)\n",
    "circuit = IQPEAltRotCXLayer(wires=num_features, num_uploads=1, num_varlayers=1, num_repeat=3, omega=torch.tensor(0.0))\n",
    "obs = qml.PauliZ(0)\n",
    "model = HamiltonianLayer(circuit, observables=[obs], qdevice=qdev, interface=interface, diff_method=diff_method)\n",
    "drawer = qml.draw(model.qnode, show_all_wires=True, expansion_strategy=\"device\")\n",
    "x = torch.randn(num_features)\n",
    "print(drawer(x))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from torch.optim import Adam\n",
    "from qulearn.trainer import SupervisedTrainer\n",
    "import logging\n",
    "opt = Adam(model.parameters(), lr=0.1, amsgrad=True)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "logger = logging.getLogger(\"SupTrainer\")\n",
    "logger.setLevel(level=logging.INFO)\n",
    "trainer = SupervisedTrainer(opt, loss_fn, num_epochs=50, logger=logger)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "trainer.train(model, dataloader, dataloader)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from qulearn.fat import fat_shattering_dim\n",
    "from qulearn.datagen import DataGenFat, UniformPrior\n",
    "prior = UniformPrior(sizex=num_features, seed=0)\n",
    "gamma=0.1\n",
    "datagen = DataGenFat(prior=prior, Sb=10, Sr=5, gamma=2.0*gamma, seed=0, batch_size=25)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "dim = fat_shattering_dim(model, datagen=datagen, trainer=trainer, dmin=1, dmax=100, gamma=gamma, dstep=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from qulearn.fat import check_shattering\n",
    "#check = check_shattering(model, datagen, trainer, 1, gamma=gamma)\n",
    "#print(check)\n",
    "print(dim)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(dim)\n",
    "data = datagen.gen_data(1)\n",
    "loader = datagen.data_to_loader(data, 0, 1)\n",
    "for x, y in loader:\n",
    "    print(x)\n",
    "    print(y)\n",
    "print(data[\"b\"])\n",
    "print(data[\"r\"])\n",
    "print(0.07260766+gamma*2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate x values\n",
    "x = np.linspace(-10, 10, 1000)  # Generate 1000 points between -10 and 10\n",
    "\n",
    "# Calculate y values for each function\n",
    "y_cos = np.cos(x)\n",
    "y_cos_x2 = np.cos(x**2)\n",
    "y_cos_x2 = np.cos(x) + 0.5*np.sin(4*x)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x, y_cos, label=\"cos(x)\", color=\"blue\")\n",
    "plt.plot(x, y_cos_x2, label=\"cos(x^2)\", color=\"red\")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Plot of cos(x) and cos(x^2)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from sympy import symbols, Matrix, cos, sin, exp\n",
    "from sympy.physics.quantum import TensorProduct as kron\n",
    "\n",
    "# ParameterVectors\n",
    "nx = 1\n",
    "ntheta = 3*1  # Adjust this for the number of parameters you need\n",
    "x = symbols('x0:{}'.format(ntheta))\n",
    "theta = symbols('theta0:{}'.format(ntheta))\n",
    "omega = symbols('omega')\n",
    "\n",
    "# Initial state |0>\n",
    "ket_0 = Matrix([[1], [0]])\n",
    "\n",
    "\n",
    "# Non-Parametric Gates\n",
    "H = 1/2**0.5 * Matrix([\n",
    "    [1, 1],\n",
    "    [1, -1]\n",
    "])\n",
    "Z = Matrix([\n",
    "    [1, 0],\n",
    "    [0, -1]\n",
    "])\n",
    "CX = Matrix([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 0, 1],\n",
    "    [0, 0, 1, 0]\n",
    "])\n",
    "I = Matrix.eye(2)\n",
    "\n",
    "# Parametric Gates\n",
    "RZ = Matrix([\n",
    "    [exp(-1j*x[0]/2), 0],\n",
    "    [0, exp(1j*x[0]/2)]\n",
    "])\n",
    "U3 = Matrix([\n",
    "    [cos(theta[0]/2), -exp(1j*theta[2])*sin(theta[0]/2)],\n",
    "    [exp(1j*theta[1])*sin(theta[0]/2), exp(1j*(theta[1] + theta[2]))*cos(theta[0]/2)]\n",
    "])\n",
    "#RZZ = exp(-1j*theta/2*kron(Z, Z))\n",
    "\n",
    "# Apply the Rz gate to the initial state\n",
    "psi = U3 * RZ * H * ket_0\n",
    "\n",
    "# Compute the expectation value of Z\n",
    "expectation = (psi.H * Z * psi)\n",
    "expectation[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from sympy import symbols, Matrix, cos, sin, exp\n",
    "from sympy.physics.quantum import TensorProduct as kron\n",
    "\n",
    "# ParameterVectors\n",
    "nx = 1\n",
    "ntheta = 3*2  # Adjust this for the number of parameters you need\n",
    "x = symbols('x0:{}'.format(ntheta))\n",
    "theta = symbols('theta0:{}'.format(ntheta))\n",
    "omega = symbols('omega')\n",
    "\n",
    "# Initial state |0>\n",
    "ket_0 = Matrix([[1], [0]])\n",
    "\n",
    "\n",
    "# Non-Parametric Gates\n",
    "H = 1/2**0.5 * Matrix([\n",
    "    [1, 1],\n",
    "    [1, -1]\n",
    "])\n",
    "Z = Matrix([\n",
    "    [1, 0],\n",
    "    [0, -1]\n",
    "])\n",
    "CX = Matrix([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 0, 1],\n",
    "    [0, 0, 1, 0]\n",
    "])\n",
    "I = Matrix.eye(2)\n",
    "\n",
    "# Parametric Gates\n",
    "RZ_1 = Matrix([\n",
    "    [exp(-1j*x[0]/2), 0],\n",
    "    [0, exp(1j*x[0]/2)]\n",
    "])\n",
    "RZ_2 = Matrix([\n",
    "    [exp(-1j*(2**(omega))*x[1]/2), 0],\n",
    "    [0, exp(1j*(2**(omega))*x[1]/2)]\n",
    "])\n",
    "\n",
    "offset = 0\n",
    "U3_1 = Matrix([\n",
    "    [cos(theta[0+offset]/2), -exp(1j*theta[2+offset])*sin(theta[0+offset]/2)],\n",
    "    [exp(1j*theta[1+offset])*sin(theta[0+offset]/2), exp(1j*(theta[1+offset] + theta[2+offset]))*cos(theta[0+offset]/2)]\n",
    "])\n",
    "offset = 3\n",
    "U3_2 = Matrix([\n",
    "    [cos(theta[0+offset]/2), -exp(1j*theta[2+offset])*sin(theta[0+offset]/2)],\n",
    "    [exp(1j*theta[1+offset])*sin(theta[0+offset]/2), exp(1j*(theta[1+offset] + theta[2+offset]))*cos(theta[0+offset]/2)]\n",
    "])\n",
    "#RZZ = exp(-1j*theta/2*kron(Z, Z))\n",
    "\n",
    "# Apply the Rz gate to the initial state\n",
    "psi = U3_2 * RZ_2 * H * U3_1 * RZ_1 * H * ket_0\n",
    "\n",
    "# Compute the expectation value of Z\n",
    "expectation = (psi.H * Z * psi)\n",
    "expectation[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from sympy import symbols, Matrix, cos, sin, exp\n",
    "from sympy.physics.quantum import TensorProduct as kron\n",
    "\n",
    "# ParameterVectors\n",
    "nx = 1\n",
    "ntheta = 3*4  # Adjust this for the number of parameters you need\n",
    "x = symbols('x0:{}'.format(ntheta))\n",
    "theta = symbols('theta0:{}'.format(ntheta))\n",
    "omega = symbols('omega')\n",
    "\n",
    "# Initial state |0>\n",
    "ket_0 = Matrix([[1], [0]])\n",
    "\n",
    "\n",
    "# Non-Parametric Gates\n",
    "H = 1/2**0.5 * Matrix([\n",
    "    [1, 1],\n",
    "    [1, -1]\n",
    "])\n",
    "Z = Matrix([\n",
    "    [1, 0],\n",
    "    [0, -1]\n",
    "])\n",
    "CX = Matrix([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 0, 1],\n",
    "    [0, 0, 1, 0]\n",
    "])\n",
    "I = Matrix.eye(2)\n",
    "\n",
    "# Parametric Gates\n",
    "RZ_1 = Matrix([\n",
    "    [exp(-1j*x[0]/2), 0],\n",
    "    [0, exp(1j*x[0]/2)]\n",
    "])\n",
    "RZ_2 = Matrix([\n",
    "    [exp(-1j*(2**(omega))*x[1]/2), 0],\n",
    "    [0, exp(1j*(2**(omega))*x[1]/2)]\n",
    "])\n",
    "\n",
    "offset = 0\n",
    "U3_1 = Matrix([\n",
    "    [cos(theta[0+offset]/2), -exp(1j*theta[2+offset])*sin(theta[0+offset]/2)],\n",
    "    [exp(1j*theta[1+offset])*sin(theta[0+offset]/2), exp(1j*(theta[1+offset] + theta[2+offset]))*cos(theta[0+offset]/2)]\n",
    "])\n",
    "offset = 3\n",
    "U3_2 = Matrix([\n",
    "    [cos(theta[0+offset]/2), -exp(1j*theta[2+offset])*sin(theta[0+offset]/2)],\n",
    "    [exp(1j*theta[1+offset])*sin(theta[0+offset]/2), exp(1j*(theta[1+offset] + theta[2+offset]))*cos(theta[0+offset]/2)]\n",
    "])\n",
    "offset = 6\n",
    "U3_3 = Matrix([\n",
    "    [cos(theta[0+offset]/2), -exp(1j*theta[2+offset])*sin(theta[0+offset]/2)],\n",
    "    [exp(1j*theta[1+offset])*sin(theta[0+offset]/2), exp(1j*(theta[1+offset] + theta[2+offset]))*cos(theta[0+offset]/2)]\n",
    "])\n",
    "offset = 9\n",
    "U3_4 = Matrix([\n",
    "    [cos(theta[0+offset]/2), -exp(1j*theta[2+offset])*sin(theta[0+offset]/2)],\n",
    "    [exp(1j*theta[1+offset])*sin(theta[0+offset]/2), exp(1j*(theta[1+offset] + theta[2+offset]))*cos(theta[0+offset]/2)]\n",
    "])\n",
    "#RZZ = exp(-1j*theta/2*kron(Z, Z))\n",
    "\n",
    "# Apply the Rz gate to the initial state\n",
    "psi = kron(U3_3, U3_4) * CX * kron(U3_1, U3_2) * kron(RZ_1, RZ_2) * kron(H, H) * (kron(ket_0, ket_0))\n",
    "\n",
    "# Compute the expectation value of Z\n",
    "expectation = (psi.H * kron(Z, I) * psi)\n",
    "expectation[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def solve_heat_conduction_radially(R, k, dr):\n",
    "    # Define the discretized radial domain\n",
    "    r = np.arange(0, R + dr, dr)\n",
    "    n = len(r)\n",
    "    \n",
    "    # Initialize the temperature field\n",
    "    T = np.zeros(n)\n",
    "    \n",
    "    # Source term\n",
    "    S = np.cos(r)\n",
    "    \n",
    "    # Finite difference matrix (using central differences)\n",
    "    A = np.zeros((n, n))\n",
    "    for i in range(1, n - 1):  # skip the center (defined as 0) and boundary (fixed by Dirichlet)\n",
    "        A[i, i - 1] = k / (dr**2) - k / (2 * r[i] * dr)\n",
    "        A[i, i] = -2 * k / (dr**2)\n",
    "        A[i, i + 1] = k / (dr**2) + k / (2 * r[i] * dr)\n",
    "    \n",
    "    # Neumann at the center (ensures smoothness at r=0)\n",
    "    A[0, 0] = -1\n",
    "    A[0, 1] = 1\n",
    "    \n",
    "    # Dirichlet at the boundary (set as 0, can be modified)\n",
    "    A[-1, -1] = 1\n",
    "    \n",
    "    # Solve the linear system\n",
    "    T = np.linalg.solve(A, S)\n",
    "    \n",
    "    return r, T\n",
    "\n",
    "# Parameters\n",
    "R = 1.0  # radius of the domain\n",
    "k = 1.0  # thermal conductivity\n",
    "dr = 0.01  # radial step size\n",
    "\n",
    "r, T = solve_heat_conduction_radially(R, k, dr)\n",
    "\n",
    "# Compute average temperature over the domain\n",
    "average_temperature = np.trapz(2 * np.pi * r * T, r) / (np.pi * R**2)\n",
    "\n",
    "print(\"Average Temperature:\", average_temperature)\n",
    "\n",
    "# Plot the solution\n",
    "plt.plot(r, T, label='Temperature Profile')\n",
    "plt.xlabel('Radial Distance')\n",
    "plt.ylabel('Temperature')\n",
    "plt.title('Steady-state Temperature Distribution in a Circle')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the function\n",
    "def f(x, low=1.0, high=30.0):\n",
    "    return np.sin(low*x) + 0.5 * np.sin(high*x)\n",
    "\n",
    "# Generate x values\n",
    "x = np.linspace(0, 2*np.pi, 1000)\n",
    "\n",
    "# Compute y values\n",
    "y = f(x)\n",
    "\n",
    "# Plot the function\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y)\n",
    "plt.title('Function with Two Fourier Frequencies')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "import pennylane as qml\n",
    "from qulearn.qlayer import AltRotCXLayer, ParallelEntangledIQPEncoding, ParallelIQPEncoding, MeasurementLayer, MeasurementType, IQPERYCZLayer, RYCZLayer, IQPEAltRotCXLayer, HadamardLayer, HamiltonianLayer, IQPEmbeddingLayer, AltRXCXLayer\n",
    "from qulearn.qkernel import QKernel\n",
    "from qulearn.trainer import RidgeRegression\n",
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "num_features = 1\n",
    "num_feature_repeat = 5\n",
    "wires = num_features*num_feature_repeat\n",
    "#embed = ParallelEntangledIQPEncoding(wires, num_features=num_features, omega=1.0, n_repeat=1)\n",
    "embed = ParallelIQPEncoding(wires, num_features=num_features, omega=1.0, n_repeat=1)\n",
    "#embedvar = IQPEAltRotCXLayer(wires=wires, num_uploads=1, num_varlayers=1, num_repeat=3, omega=1.0)\n",
    "#embed = IQPEmbeddingLayer(wires=wires, n_repeat=1)\n",
    "hads = HadamardLayer(wires=wires)\n",
    "var = AltRotCXLayer(wires=wires, n_layers=3)\n",
    "#var = AltRXCXLayer(wires=wires, n_layers=10)\n",
    "#embed = IQPERYCZLayer(wires=wires, num_uploads=1, num_varlayers=0, num_repeat=2, omega=1.0)\n",
    "#embed = ParallelIQPEncoding(wires, num_features=1, omega=1.0, n_repeat=2)\n",
    "#obs = [qml.Identity(0), qml.PauliX(0), qml.PauliX(1), qml.PauliX(2), qml.PauliX(3),\n",
    "#       qml.PauliX(4), qml.PauliX(0) @ qml.PauliX(1) @ qml.PauliX(2) @ qml.PauliX(3) @ qml.PauliX(4)]\n",
    "obs = [qml.Identity(0), qml.PauliZ(0)]\n",
    "#obs = [qml.Identity(0), qml.PauliX(0)]\n",
    "num_samples = 100\n",
    "#X_train = torch.randn((num_samples, num_features))\n",
    "labels = torch.randn((num_samples, 1))\n",
    "\n",
    "qdevice = qml.device(\"lightning.qubit\", wires=wires, shots=None)\n",
    "interface = \"torch\"\n",
    "diff_method = \"adjoint\"\n",
    "\n",
    "#model = QKernel(embed, X_train)\n",
    "#model = MeasurementLayer(embedvar, measurement_type=MeasurementType.Entropy, observable=obs)\n",
    "#model = MeasurementLayer(embedvar, measurement_type=MeasurementType.Expectation, observable=obs)\n",
    "#level = 5\n",
    "#wires = level\n",
    "#embed = QTT1DEncoding(wires, level=level)\n",
    "#var = AltRotCXLayer(wires=wires, n_layers=3)\n",
    "#var1 = AltRotCXLayer(wires=wires, n_layers=121)\n",
    "#var2 = AltRotCXLayer(wires=wires, n_layers=5)\n",
    "#var3 = AltRotCXLayer(wires=wires, n_layers=5)\n",
    "#var4 = AltRotCXLayer(wires=wires, n_layers=5)\n",
    "embed1 = IQPEmbeddingLayer(wires=wires, n_repeat=1)\n",
    "embed2 = IQPEmbeddingLayer(wires=wires, n_repeat=1)\n",
    "embed3 = IQPEmbeddingLayer(wires=wires, n_repeat=1)\n",
    "embed4 = IQPEmbeddingLayer(wires=wires, n_repeat=1)\n",
    "embed5 = IQPEmbeddingLayer(wires=wires, n_repeat=1)\n",
    "model = HamiltonianLayer(embed, var, observables=obs)\n",
    "drawer = qml.draw(model.qnode, show_all_wires=True, expansion_strategy=\"device\")\n",
    "x1 = 2*torch.rand(num_features) - 1\n",
    "x2 = torch.tensor([0.2, 0.3])\n",
    "print(x1)\n",
    "print(drawer(x1))\n",
    "#model.subwires = [0, 1]\n",
    "#print(model(x1))\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "print(qml.specs(model.qnode)(x1))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "drawer = qml.draw_mpl(model.qnode, show_all_wires=True, expansion_strategy=\"device\")\n",
    "print(drawer(x1))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "source": [
    "from torch import nn\n",
    "class ExtendedModel(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(ExtendedModel, self).__init__()\n",
    "        self.original_model = original_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply arcsin to each component of X\n",
    "        #x = torch.exp(-((x) ** 2) / (2 * 1 ** 2))\n",
    "        #x = torch.asin(x)\n",
    "        x = x**2\n",
    "        \n",
    "        # Pass the transformed X to the original model\n",
    "        return self.original_model(x)\n",
    "    \n",
    "extended = ExtendedModel(model)\n",
    "model = extended"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "\n",
    "class ClassicalSurrogate(nn.Module):\n",
    "    def __init__(self, z_function, omega_spectrum):\n",
    "        super(ClassicalSurrogate, self).__init__()\n",
    "        self.omegas = omega_spectrum\n",
    "        \n",
    "        self.alpha = nn.Parameter(torch.empty(self.omegas.size(0), 1, dtype=torch.float64))\n",
    "        self.beta = nn.Parameter(torch.empty(self.omegas.size(0), 1, dtype=torch.float64))\n",
    "        nn.init.normal_(self.alpha)\n",
    "        nn.init.normal_(self.beta)\n",
    "        self.z_function = z_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_x = self.z_function(x)\n",
    "        \n",
    "        # Compute dot product\n",
    "        dot_products = torch.mm(z_x, self.omegas.t())\n",
    "        \n",
    "        # Compute cosine and sine values\n",
    "        cos_values = torch.cos(dot_products)\n",
    "        sin_values = torch.sin(dot_products)\n",
    "        \n",
    "        # Weighted sum of cosine and sine values for each sample\n",
    "        outputs = torch.sum(self.alpha.squeeze() * cos_values + self.beta.squeeze() * sin_values, dim=1)\n",
    "        return outputs.unsqueeze(1)\n",
    "\n",
    "# Example usage:\n",
    "def id_z(x):\n",
    "    # For demonstration, we're returning x. Replace this with any required transformation.\n",
    "    return x\n",
    "\n",
    "# Assuming z(x) is a 3-dimensional vector, and each component can take values [1.0, 2.0] or [2.0, 3.0] or [1.0, 3.0]\n",
    "L = 32\n",
    "omega_spectrum = list(range(-32, 33))\n",
    "#all_combinations = list(itertools.product(*omega_spectrum))\n",
    "all_combinations = omega_spectrum\n",
    "all_combinations = torch.tensor(all_combinations, dtype=torch.float64).reshape(65, 1)\n",
    "model = ClassicalSurrogate(id_z, all_combinations)\n",
    "\n",
    "# Example forward pass:\n",
    "x = torch.tensor([[1.0], [2.0]], dtype=torch.float64)\n",
    "output = model(x)\n",
    "print(output)\n",
    "print(output.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# First, let's import necessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_dim = num_features\n",
    "# Define the LeakyReLU network\n",
    "class LeakyReLUNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LeakyReLUNet, self).__init__()\n",
    "        # Define layers of the network (You can adjust the number of neurons as needed)\n",
    "        dim = 8\n",
    "        self.fc1 = nn.Linear(input_dim, dim, dtype=torch.float64)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(dim, dim, dtype=torch.float64)         # Second fully connected layer\n",
    "        self.fc3 = nn.Linear(dim, dim, dtype=torch.float64)\n",
    "        self.fc4 = nn.Linear(dim, dim, dtype=torch.float64)\n",
    "        self.fc5 = nn.Linear(dim, 1, dtype=torch.float64)          # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = F.leaky_relu(self.fc4(x))\n",
    "        return self.fc5(x)\n",
    "\n",
    "# Initialize the network\n",
    "model = LeakyReLUNet(input_dim)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n",
    "print([p.shape for p in model.parameters() if p.requires_grad])\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "print(model(x).shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from torchviz import make_dot\n",
    "# Create a random tensor to represent a sample input\n",
    "x = torch.randn(1, input_dim)  # Assuming a batch size of 1\n",
    "y = model(x)\n",
    "\n",
    "# Visualize the network\n",
    "make_dot(y, params=dict(model.named_parameters())).render(\"network\", format=\"png\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from qulearn.qlayer import MeasurementType\n",
    "x = torch.rand(num_features)*2-1\n",
    "subsystems = [list(range(i+1)) for i in range(wires-1)]\n",
    "entropies = []\n",
    "model.measurement_type = MeasurementType.Entropy\n",
    "for subsystem in subsystems:\n",
    "    model.subwires = subsystem\n",
    "    entropies.append(model(x).item())\n",
    "max_entropies = [np.log2(min(2**len(subsystem), 2**(wires - len(subsystem)))) for subsystem in subsystems]\n",
    "print(x)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "x = list(range(wires-1))\n",
    "plt.plot(x, entropies, '-o', label='Computed Entropy')\n",
    "plt.plot(x, max_entropies, '-o', label='Maximum Entropy')\n",
    "\n",
    "plt.xlabel('Subsystem')\n",
    "plt.ylabel('VN Entropy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Computed vs Maximum Entropy')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "# Generate a sample of inputs\n",
    "X = torch.linspace(-1, 1, 1000).reshape(-1, 1)\n",
    "\n",
    "# Define a function to add Gaussian noise\n",
    "def add_gaussian_noise(tensor, mean=0., std=0.01):\n",
    "    return tensor + torch.randn(tensor.size()) * std + mean\n",
    "\n",
    "# 1. Linear Relationship\n",
    "def linear(X, m=2, c=3):\n",
    "    return m * X + c\n",
    "Y_linear = add_gaussian_noise(linear(X))\n",
    "\n",
    "# 2. Polynomial Relationship (Quadratic for demonstration)\n",
    "def polynomial(X, a=3, b=2, c=-5):\n",
    "    return a * X**2 + b * X + c\n",
    "Y_polynomial = add_gaussian_noise(polynomial(X))\n",
    "\n",
    "# 3. Exponential Relationship\n",
    "def exponential(X, a=2, b=0.5):\n",
    "    return a * torch.exp(b * X)\n",
    "Y_exponential = add_gaussian_noise(exponential(X))\n",
    "\n",
    "# 4. Logarithmic Relationship\n",
    "def logarithmic(X, a=3, b=2):\n",
    "    return a * torch.log(X) + b\n",
    "Y_logarithmic = add_gaussian_noise(logarithmic(X))\n",
    "\n",
    "# 5. Trigonometric Relationship\n",
    "def trigonometric(X, a=5, b=2, c=0.5):\n",
    "    return a * torch.sin(b * X + c)\n",
    "Y_trigonometric = add_gaussian_noise(trigonometric(X))\n",
    "\n",
    "# 6. Power-law Relationship\n",
    "def power_law(X, a=2, b=1.5):\n",
    "    return a * X**b\n",
    "Y_power_law = add_gaussian_noise(power_law(X))\n",
    "\n",
    "# 7. Sigmoidal Relationship\n",
    "def sigmoidal(X, L=1, k=0.5, x0=5):\n",
    "    return L / (1 + torch.exp(-k * (X - x0)))\n",
    "Y_sigmoidal = add_gaussian_noise(sigmoidal(X))\n",
    "\n",
    "# 8. Gaussian Relationship\n",
    "def gaussian(X, a=1., b=0, c=1):\n",
    "    return a * torch.exp(-((X - b) ** 2) / (2 * c ** 2))\n",
    "Y_gaussian = add_gaussian_noise(gaussian(X))\n",
    "\n",
    "# 9. Step Function\n",
    "def step_function(X, threshold1=-0.5, threshold2=0.5, low_value=-1, mid_value=1, high_value=0):\n",
    "    condition1 = (X < threshold1)\n",
    "    condition2 = (X >= threshold1) & (X < threshold2)\n",
    "    condition3 = (X >= threshold2)\n",
    "\n",
    "    values = torch.zeros_like(X)\n",
    "    values[condition1] = low_value\n",
    "    values[condition2] = mid_value\n",
    "    values[condition3] = high_value\n",
    "    \n",
    "    return values\n",
    "\n",
    "Y_step = add_gaussian_noise(step_function(X))\n",
    "\n",
    "def piecewise_linear_3segments(X, a=-1, b=1, m1=2, c1=0, m2=-3, m3=1):\n",
    "    # Define linear functions for each segment\n",
    "    linear_before_a = m1 * X + c1\n",
    "    linear_between_a_b = m1 * a + c1 + m2 * (X - a)\n",
    "    linear_after_b = m1 * a + c1 + m2 * (b - a) + m3 * (X - b)\n",
    "    \n",
    "    # Apply conditions for each segment\n",
    "    conditions = [(X < a), (X >= a) & (X < b), (X >= b)]\n",
    "    functions = [linear_before_a, linear_between_a_b, linear_after_b]\n",
    "\n",
    "    output = torch.zeros_like(X)\n",
    "    for condition, function in zip(conditions, functions):\n",
    "        output[condition] = function[condition]\n",
    "    \n",
    "    return output\n",
    "Y_piecewise_spiked = add_gaussian_noise(piecewise_linear_3segments(X))\n",
    "\n",
    "def multidimstep(X, a=1.0):\n",
    "    # Extract the first and second features\n",
    "    X0 = X[:, 0]\n",
    "    X1 = X[:, 1]\n",
    "\n",
    "    # Check the condition X[1] > a * X[0]\n",
    "    condition = X1 > a * X0\n",
    "\n",
    "    # Initialize the output tensor Y\n",
    "    Y = torch.zeros_like(X1)\n",
    "\n",
    "    # Set Y values based on the condition\n",
    "    Y[condition] = 1  # Set to +1 where condition is True\n",
    "    Y[~condition] = -1  # Set to -1 where condition is False\n",
    "    \n",
    "    return Y.view(-1, 1)  # Reshape to make it a column vector\n",
    "\n",
    "def high_low(X, low=1.0, high=30.0):\n",
    "    return torch.sin(low * X) + 0.5 * torch.sin(high * X)\n",
    "\n",
    "Y_high_low = add_gaussian_noise(high_low(X))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Create the scatter plot\n",
    "plt.plot(X, Y_gaussian)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "source": [
    "import torch\n",
    "import logging\n",
    "from torch.optim import Adam\n",
    "from qulearn.trainer import SupervisedTrainer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Data\n",
    "N_train = 100\n",
    "N_valid = 10\n",
    "batch_size=25\n",
    "X_train = torch.linspace(-1, 1, N_train, dtype=torch.float64).reshape(-1, 1)\n",
    "#X_train = torch.rand((N_train, num_features), dtype=torch.float64)\n",
    "Y_train = add_gaussian_noise(gaussian(X_train))\n",
    "X_valid = torch.linspace(-1, 1, N_valid, dtype=torch.float64).reshape(-1, 1)\n",
    "#X_valid=  torch.rand((N_valid, num_features), dtype=torch.float64)\n",
    "Y_valid = gaussian(X_valid)\n",
    "data_train = TensorDataset(X_train, Y_train)\n",
    "data_valid = TensorDataset(X_valid, Y_valid)\n",
    "loader_train = DataLoader(data_train, batch_size=batch_size, shuffle=False)\n",
    "loader_valid = DataLoader(data_valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Trainer\n",
    "lr = 0.1\n",
    "optimizer = Adam(model.parameters(), lr=lr, amsgrad=True)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "logger = logging.getLogger(\"train_function\")\n",
    "logger.setLevel(level=logging.INFO)\n",
    "num_epochs = 200\n",
    "trainer = SupervisedTrainer(optimizer=optimizer,\n",
    "                            loss_fn=loss_fn,\n",
    "                            num_epochs=num_epochs,\n",
    "                            logger=logger)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "source": [
    "# Train\n",
    "trainer.train(model, train_data=loader_train, valid_data=loader_valid)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from qulearn.trainer import RidgeRegression\n",
    "lambda_reg = 1e-01\n",
    "\n",
    "loader_train = DataLoader(data_train, batch_size=N_train, shuffle=False)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "metrics = {\"mse_loss\": loss_fn}\n",
    "trainer = RidgeRegression(lambda_reg=lambda_reg, metrics=metrics, logger=logger)\n",
    "trainer.train(model, loader_train, loader_train)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "source": [
    "# Plotting\n",
    "X = torch.linspace(-1, 1, 1000, dtype=torch.float64).reshape(-1, 1)\n",
    "Y_exact = gaussian(X)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    Y_model = model(X)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(X, Y_exact, label=\"exact\", color=\"blue\")\n",
    "plt.plot(X, Y_model, label=\"predicted\", color=\"red\")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Exact vs. Model\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# assuming model is your pytorch model\n",
    "# model = ...\n",
    "\n",
    "# Generate 100 random inputs between -1 and 1\n",
    "inputs = torch.linspace(-2, 2, 500)\n",
    "inputs = inputs.unsqueeze(-1)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient calculations\n",
    "with torch.no_grad():\n",
    "    # Run the inputs through the model\n",
    "    outputs = model(inputs)\n",
    "\n",
    "# Convert tensors to numpy arrays for plotting\n",
    "inputs = inputs.numpy()\n",
    "outputs = outputs.numpy()\n",
    "\n",
    "# Create the scatter plot\n",
    "#plt.scatter(inputs, outputs)\n",
    "plt.plot(inputs, outputs)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "outputs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/mazen/Research/QC/QuLearn/examples/compare_models\")\n",
    "from model_builder import QNNModel, QNNStatModel"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "model = QNNStatModel(num_features=1, num_reuploads=1, num_varlayers=1, num_repeats=1, omega=0.0, double_wires=False, id=\"0\")\n",
    "for p in model.parameters():\n",
    "    print(p)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "drawer = qml.draw(model.qnn.qnode, show_all_wires=True, expansion_strategy=\"device\")\n",
    "x = torch.randn(1, 1)\n",
    "print(model(x))\n",
    "print(drawer(x))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# assuming model is your PyTorch model\n",
    "# model = ...\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient calculations\n",
    "with torch.no_grad():\n",
    "    # Define input range\n",
    "    x = np.linspace(-2, 2, 50)\n",
    "    y = np.linspace(-2, 2, 50)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "\n",
    "    # Combine inputs and reshape for model input\n",
    "    inputs = np.array([X.flatten(), Y.flatten()]).T\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float64)\n",
    "\n",
    "    # Compute outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        #outputs = multidimstep(inputs)\n",
    "\n",
    "    # Convert tensors to numpy arrays for plotting\n",
    "    inputs = inputs.numpy()\n",
    "    outputs = outputs.numpy()\n",
    "\n",
    "    # Reshape output for heatmap\n",
    "    Z = outputs.reshape(50, 50)\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.imshow(Z, extent=[-2, 2, -2, 2], origin='lower')\n",
    "    plt.colorbar(label='Model output')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Create a 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X, Y, Z, cmap='viridis')\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('Model output')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from scipy.stats import qmc\n",
    "\n",
    "def generate_lhs_samples(n_samples, n_dims, lower_bound, upper_bound):\n",
    "    \"\"\"Generates Latin Hypercube Samples in the specified range.\"\"\"\n",
    "    sampler = qmc.LatinHypercube(d=n_dims)\n",
    "    sample = sampler.random(n=n_samples)\n",
    "    return lower_bound + sample * (upper_bound - lower_bound)\n",
    "\n",
    "import torch\n",
    "\n",
    "def generate_model_samples(model, n_samples):\n",
    "    \"\"\"Generates a list of lists of model parameters, sampled using LHS.\"\"\"\n",
    "    model_parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "\n",
    "    lower_bound = -2 * np.pi\n",
    "    upper_bound = 2 * np.pi\n",
    "    samples = []\n",
    "    for p in model_parameters:\n",
    "        n_dims = p.numel()\n",
    "        sample_parameter = generate_lhs_samples(n_samples=n_samples, n_dims=n_dims, lower_bound=lower_bound, upper_bound=upper_bound)\n",
    "        samples.append(sample_parameter)\n",
    "    parameter_list = [[torch.tensor(samples[j][i], device=None, dtype=None) for j in range(len(samples))] for i in range(n_samples)]\n",
    "\n",
    "    return parameter_list"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "test = generate_model_samples(model, 5)\n",
    "print(len(test))\n",
    "for t in test:\n",
    "    print(len(t))\n",
    "    for x in t:\n",
    "        print(x.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "for input, label in dataloader:\n",
    "    print(input)\n",
    "    print(label)\n",
    "    break"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(level=logging.INFO)\n",
    "logger.info(\"hello world\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from qulearn.qlayer import IQPEmbeddingLayer\n",
    "from qulearn.qlayer import QEvalType\n",
    "import torch\n",
    "import pennylane as qml\n",
    "from qulearn.observable import parity_all_hamiltonian \n",
    "\n",
    "numq = 3\n",
    "wires = range(numq)\n",
    "qdev = qml.device(\"lightning.qubit\", wires=numq, shots=None)\n",
    "@qml.qnode(qdev)\n",
    "def qnode():\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "out = qnode()\n",
    "print(out)\n",
    "print(qdev.shots)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import math\n",
    "t = torch.nn.Parameter(torch.zeros(3, 3))\n",
    "print(t)\n",
    "torch.nn.init.uniform_(t, a=0.0, b=2*math.pi)\n",
    "print(t)\n",
    "a = [0, 1]\n",
    "b = [0, 2]\n",
    "print(a==b)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from qulearn.qlayer import IQPEmbeddingLayer, RYCZLayer, MeasurementLayer, HamiltonianLayer, MeasurementType, EmbedVarLayer, CircuitLayer\n",
    "import pennylane as qml\n",
    "import torch\n",
    "from qulearn.observable import parities_all_observables\n",
    "\n",
    "observable = qml.PauliZ(0)\n",
    "num_wires = 3\n",
    "num_reup = 1\n",
    "num_layers = 1\n",
    "num_repeat = 1\n",
    "qdev = qml.device('default.qubit', wires=num_wires, shots=None)\n",
    "\n",
    "upload_layer = IQPEmbeddingLayer(num_wires, num_reup)\n",
    "var_layer = RYCZLayer(num_wires, num_layers)\n",
    "embedvar = EmbedVarLayer(upload_layer, var_layer, n_repeat=num_repeat, omega=1.0)\n",
    "measure_layer = MeasurementLayer(embedvar, qdevice=qdev, measurement_type=MeasurementType.Expectation, observable=observable)\n",
    "obs = parities_all_observables(num_wires)\n",
    "ham_layer = HamiltonianLayer(embedvar, observables=obs, interface='torch', diff_method='backprop')\n",
    "#ham_layer = HamiltonianLayer(upload_layer, observables=obs)\n",
    "\n",
    "x = torch.randn(3, num_wires)\n",
    "drawer = qml.draw_mpl(ham_layer.qnode, show_all_wires=True, expansion_strategy=\"device\")\n",
    "drawer(x)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "y = torch.cat((x, x), dim=1)\n",
    "print(x)\n",
    "print(y)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from qulearn.utils import all_bin_sequences\n",
    "from qulearn.observable import sequence2parity_observable\n",
    "from itertools import combinations\n",
    "all = all_bin_sequences(3)\n",
    "pairs = list(combinations(range(3),2 ))\n",
    "print(all)\n",
    "print(pairs)\n",
    "all_obs = sequence2parity_observable(all)\n",
    "pairs_obs = sequence2parity_observable(pairs)\n",
    "\n",
    "print(all_obs)\n",
    "print(pairs_obs)\n",
    "all_obs[0].name"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(tuple(range(3)))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "for p in ham_layer.parameters():\n",
    "    print(p)\n",
    "print(ham_layer.observable.coeffs)\n",
    "print(ham_layer.observable)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "tmp = torch.tensor([1., -1., 0.5])\n",
    "print(tmp.numel())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from qulearn.fim import empirical_fim, compute_fims, mc_integrate_idfim_det\n",
    "\n",
    "FIM = empirical_fim(measure_layer, x)\n",
    "for p in measure_layer.parameters():\n",
    "    print(p)\n",
    "\n",
    "plist = []\n",
    "p1 = torch.randn(4, 3)\n",
    "p2 = torch.randn(4, 3, 2, 2)\n",
    "for i in range(4):\n",
    "    plist.append([p1[i], p2[i]])\n",
    "\n",
    "FIMs = compute_fims(measure_layer, x, plist)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "integral = mc_integrate_idfim_det(FIMs, 1.0, 1.0)\n",
    "print(integral)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "lin = torch.nn.Linear(3, 1)\n",
    "print(lin(x))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "y1 = upload_layer(x)\n",
    "y2 = var_layer(x)\n",
    "#y3 = measure_layer(x)\n",
    "y4 = ham_layer(x)\n",
    "\n",
    "print(y3)\n",
    "print(y4)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "print(y3)\n",
    "log_prob = torch.log(y3)\n",
    "print(torch.log(y3))\n",
    "print(x.shape)\n",
    "\n",
    "measure_layer.zero_grad()\n",
    "log_prob.backward(retain_graph=True)\n",
    "grad_list = [\n",
    "    p.grad.view(-1)\n",
    "    for p in measure_layer.parameters()\n",
    "    if p.requires_grad and p.grad is not None\n",
    "]\n",
    "grad = torch.cat(grad_list)\n",
    "prod = torch.outer(grad, grad)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "for key, val in measure_layer.state_dict().items():\n",
    "    print(key)\n",
    "    print(val)\n",
    "    print(\"====\")\n",
    "\n",
    "print(\"************************************\")\n",
    "\n",
    "for key, val in ham_layer.state_dict().items():\n",
    "    print(key)\n",
    "    print(val)\n",
    "    print(\"====\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from torch import nn\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ll_in = nn.Linear(3, 3, dtype=torch.float64)\n",
    "        self.meas = ham_layer\n",
    "        self.ll_out = nn.Linear(1, 1, dtype=torch.float64)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        y = self.ll_in(x)\n",
    "        y = self.meas(y)\n",
    "        y = self.ll_out(y)\n",
    "\n",
    "        return y\n",
    "    \n",
    "hybrid = HybridModel()\n",
    "x = torch.rand(3, dtype=torch.float64)\n",
    "y = hybrid(x)\n",
    "print(y)\n",
    "for key, val in hybrid.state_dict().items():\n",
    "    print(key)\n",
    "    print(val)\n",
    "    print(\"======\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(y)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "for key, val in measure_layer.state_dict().items():\n",
    "    print(key)\n",
    "    print(val)\n",
    "    print(\"=====\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from enum import Enum\n",
    "\n",
    "class MyType(Enum):\n",
    "\n",
    "    TypeA = 0\n",
    "    TypeB = 1\n",
    "\n",
    "x = MyType.TypeA\n",
    "y = \"blub\"\n",
    "\n",
    "if x == MyType.TypeA:\n",
    "    print(x)\n",
    "\n",
    "if not isinstance(y, MyType):\n",
    "    raise NotImplementedError(\"QEvalType not implemented\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "model = Linear(3, 1)\n",
    "X = torch.tensor([[0.1, 1.1, -2.2], [0.6, 4.1, -3.2], [-0.1, -2.1, -2.2]])\n",
    "Y = model(X)\n",
    "print(Y.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"runs/test\")\n",
    "for i in range(10):\n",
    "    val = 1e-01\n",
    "    writer.add_scalar(\"foobar1\", val, i)\n",
    "    writer.add_scalars(\"loss\", {\"train\": val, \"valid\": val+1.0}, i)\n",
    "writer.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import pennylane as qml\n",
    "from qml_mor.models import IQPEReuploadSU2Parity, ModelType\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "num_samples=1\n",
    "num_qubits = 3\n",
    "num_reups = 1\n",
    "num_layers = 1\n",
    "sizex = num_qubits\n",
    "\n",
    "omega = 0.0\n",
    "init_theta = torch.randn(num_reups, num_qubits, requires_grad=True)\n",
    "theta = torch.randn(\n",
    "    num_reups, num_layers, num_qubits - 1, 2, requires_grad=True\n",
    ")\n",
    "W = torch.randn(2**num_qubits, requires_grad=True)\n",
    "\n",
    "params = [init_theta, theta, W]\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=num_qubits, shots=None)\n",
    "\n",
    "model = IQPEReuploadSU2Parity(dev, params, model_type=ModelType.Expectation)\n",
    "X = torch.randn(10, 3)\n",
    "Ypred = model(X)\n",
    "print(Ypred)\n",
    "print(Ypred.shape)\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "test = loss(Ypred, Ypred)\n",
    "print(test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from torch.nn import Parameter\n",
    "P = model.parameters()\n",
    "for p in P:\n",
    "    print(p)\n",
    "print(\"====================\")\n",
    "save = model.state_dict().copy()\n",
    "\n",
    "init_theta = torch.randn(num_reups, num_qubits, requires_grad=True)\n",
    "theta = torch.randn(\n",
    "    num_reups, num_layers, num_qubits - 1, 2, requires_grad=True\n",
    ")\n",
    "W = torch.randn(2**num_qubits, requires_grad=True)\n",
    "\n",
    "params = [init_theta, theta, W]\n",
    "\n",
    "model = IQPEReuploadSU2Parity(dev, params)\n",
    "\n",
    "P = model.parameters()\n",
    "for p in P:\n",
    "    print(p)\n",
    "print(\"====================\")\n",
    "model.load_state_dict(save)\n",
    "P = model.parameters()\n",
    "for p in P:\n",
    "    print(p)\n",
    "print(\"====================\")\n",
    "print(save)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "from qulearn.qlayer import IQPEmbeddingLayer, RYCZLayer, MeasurementLayer, HamiltonianLayer, MeasurementType, EmbedVarLayer, CircuitLayer\n",
    "import pennylane as qml\n",
    "import torch\n",
    "from qulearn.observable import parities_all_observables\n",
    "\n",
    "observable = qml.PauliZ(0)\n",
    "num_wires = 3\n",
    "num_reup = 1\n",
    "num_layers = 1\n",
    "num_repeat = 3\n",
    "qdev = qml.device('default.qubit', wires=num_wires, shots=None)\n",
    "\n",
    "upload_layer = IQPEmbeddingLayer(num_wires, num_reup)\n",
    "var_layer = RYCZLayer(num_wires, num_layers)\n",
    "embedvar = EmbedVarLayer(upload_layer, var_layer, n_repeat=num_repeat, omega=1.0)\n",
    "obs = parities_all_observables(num_wires)\n",
    "ham_layer = HamiltonianLayer(embedvar,  qdevice=qdev, observables=obs, interface='torch', diff_method='adjoint')\n",
    "\n",
    "x = torch.zeros(3, num_wires)\n",
    "print(ham_layer(x))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "N = 100\n",
    "torch.manual_seed(0)\n",
    "X = torch.randn(N, 10, dtype=torch.float64)\n",
    "#Y = torch.randn(N, 1, dtype=torch.float64)\n",
    "A = torch.randn(10, 1, dtype=torch.float64)\n",
    "b = torch.randn(1, dtype=torch.float64)\n",
    "eps = torch.randn(N, dtype=torch.float64)*0.001\n",
    "Y = torch.matmul(X, A) + b + eps\n",
    "model = torch.nn.Linear(10, 1, bias=True, dtype=torch.float64)\n",
    "batch_size=4\n",
    "shuffle=True\n",
    "\n",
    "dataset = TensorDataset(X, Y)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from torch.optim import Adam\n",
    "from qulearn.trainer import SupervisedTrainer\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "opt = Adam(model.parameters(), lr=0.1, amsgrad=True)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "metrics = {\"L1\": loss_fn, \"L2\": loss_fn}\n",
    "writer = SummaryWriter()\n",
    "logger = logging.getLogger(\"SupTrainer\")\n",
    "logger.setLevel(level=logging.INFO)\n",
    "trainer = SupervisedTrainer(opt, loss_fn, metrics, 200, logger=logger)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "trainer.train(model, loader, loader)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%timeit\n",
    "trainer.train(ham_layer, loader, loader) # lightning + torch + adjoint (+ omp-num-threads=8)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%timeit\n",
    "trainer.train(ham_layer, loader, loader) # default + torch + backprop"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%timeit\n",
    "trainer.train(ham_layer, loader, loader) # default + torch + adjoint"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%lprun -T lprof0 -u 1.0 -f trainer._train_step trainer.train(ham_layer, loader, loader)\n",
    "writer.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from enum import Enum\n",
    "\n",
    "class MeasurementType(Enum):\n",
    "    \"\"\"Measurement type for a measurement layer.\"\"\"\n",
    "\n",
    "    \"\"\"Expectation: return expected value of observable.\"\"\"\n",
    "    Expectation\n",
    "    \"\"\"Probabilities: return vector of probabilities.\"\"\"\n",
    "    Probabilities = \"probabilities\"\n",
    "    \"\"\"Samples: return measurement samples.\"\"\"\n",
    "    Samples = \"samples\"\n",
    "\n",
    "example = MeasurementType.Expectation\n",
    "print(type(example.name))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from qml_mor.trainer import RegressionTrainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import Adam\n",
    "from datetime import datetime\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.1, amsgrad=True)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/check_{}'.format(timestamp))\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "training = RegressionTrainer(optimizer, loss_fn, writer)\n",
    "loss = training.train(model, loader, loader)\n",
    "print(loss)\n",
    "writer.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "path = \"model_bestmre\"\n",
    "state = torch.load(path)\n",
    "model.load_state_dict(state)\n",
    "Ypred = model(X)\n",
    "print(Y)\n",
    "print(\"==============\")\n",
    "print(Ypred)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/fashion_trainer_20230517_113403"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import pennylane as qml\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from qml_mor.models import IQPEReuploadSU2Parity\n",
    "from qml_mor.trainer import AdamTorch\n",
    "from qml_mor.datagen import DataGenCapacity\n",
    "\n",
    "num_samples=1\n",
    "num_qubits = 3\n",
    "num_reups = 1\n",
    "num_layers = 1\n",
    "sizex = num_qubits\n",
    "\n",
    "datagen = DataGenCapacity(sizex=sizex, num_samples=num_samples)\n",
    "\n",
    "omega = 0.0\n",
    "init_theta = torch.randn(num_reups, num_qubits, requires_grad=True)\n",
    "theta = torch.randn(\n",
    "    num_reups, num_layers, num_qubits - 1, 2, requires_grad=True\n",
    ")\n",
    "W = torch.randn(2**num_qubits, requires_grad=True)\n",
    "\n",
    "params = [init_theta, theta, W]\n",
    "\n",
    "model = IQPEReuploadSU2Parity(omega)\n",
    "dev = qml.device(\"default.qubit\", wires=num_qubits, shots=None)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def qnn_model(x, params):\n",
    "    return model.qfunction(x, params)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "num_epochs = 300\n",
    "opt = AdamTorch(params, loss_fn, num_epochs=num_epochs, amsgrad=True)\n",
    "\n",
    "N = 10\n",
    "batch_size=int(\"inf\")\n",
    "shuffle=True\n",
    "data = datagen.gen_data(N)\n",
    "X = data[\"X\"]\n",
    "Y = data[\"Y\"][0]\n",
    "dataset = TensorDataset(X, Y)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "opt_params = opt.optimize(qnn_model, loader)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Nx = X.size(0)\n",
    "y_pred = torch.stack([qnn_model(X[k], opt_params) for k in range(Nx)])\n",
    "mre = torch.mean(torch.abs((Y - y_pred) / y_pred))\n",
    "\n",
    "print(mre)\n",
    "print(loss_fn(y_pred, Y))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "X = torch.randn(5, 3)\n",
    "Y = torch.randn(5)\n",
    "batch_size = 2\n",
    "\n",
    "dataset = TensorDataset(X, Y)\n",
    "shuffle = True\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "print(X)\n",
    "print(Y)\n",
    "print(\"===========\")\n",
    "for X_, Y_ in loader:\n",
    "    print(X_.size(0))\n",
    "    print(X_)\n",
    "    print(Y_)\n",
    "    print(\"***********\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from qml_mor.datagen import DataGenRademacher, NormalPrior\n",
    "prior = NormalPrior(3, seed=0)\n",
    "radem = DataGenRademacher(prior, 2, 3, seed=None)\n",
    "data = radem.gen_data(4)\n",
    "print(data)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import pennylane as qml\n",
    "num_qubits = 3\n",
    "W = torch.randn(2**num_qubits, requires_grad=False)\n",
    "from qulearn.qlayer import parity_hamiltonian\n",
    "H = parity_hamiltonian(num_qubits, W)\n",
    "print(W)\n",
    "print(\"======\")\n",
    "Z = qml.PauliZ(wires=0)\n",
    "print(H.coeffs.requires_grad)\n",
    "W_ = torch.nn.Parameter()\n",
    "print(W_)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "from qml_mor.models import IQPEReuploadSU2Parity\n",
    "\n",
    "num_qubits  = 3\n",
    "num_reps    = 2\n",
    "num_layers  = 2\n",
    "omega       = 1.\n",
    "torch.manual_seed(0)\n",
    "x           = torch.randn(num_qubits, requires_grad=False)\n",
    "init_theta  = torch.randn(num_reps, num_qubits, requires_grad=True)\n",
    "theta       = torch.randn(num_reps, num_layers, num_qubits-1, 2, requires_grad=True)\n",
    "W           = torch.randn(2**num_qubits, requires_grad=True)\n",
    "\n",
    "params = [init_theta, theta, W]\n",
    "\n",
    "def iqpe_reupload_su2_parity(\n",
    "    x, init_theta, theta, W, omega: float = 0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Quantum function that calculates the expectation value\n",
    "    of the parity of Pauli Z operators.\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): Input tensor of shape (num_qubits,)\n",
    "        init_theta (Tensor): Initial rotation angles for each qubit,\n",
    "            of shape (reps, num_qubits)\n",
    "        theta (Tensor): Rotation angles for each layer and each qubit,\n",
    "            of shape (reps, num_layers, num_qubits-1, 2)\n",
    "        W (Tensor): Observable weights of shape (2^num_qubits,)\n",
    "        omega (float, optional): Exponential feature scaling factor. Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "        QFuncOutput: Expectation value of the parity of Pauli Z operators\n",
    "    \"\"\"\n",
    "\n",
    "    shape_init = init_theta.shape\n",
    "    shape = theta.shape\n",
    "    if len(shape_init) != 2:\n",
    "        raise ValueError(\"Initial theta must be a 2-dim tensor\")\n",
    "    if len(shape) != 4:\n",
    "        raise ValueError(\"Theta must be a 4-dim tensor\")\n",
    "\n",
    "    num_qubits = len(x)\n",
    "    reps = shape_init[0]\n",
    "    wires = range(num_qubits)\n",
    "\n",
    "    for layer in range(reps):\n",
    "        features = 2 ** (omega * layer) * x\n",
    "        initial_layer_weights = init_theta[layer]\n",
    "        weights = theta[layer]\n",
    "\n",
    "        qml.IQPEmbedding(features=features, wires=wires)\n",
    "        qml.SimplifiedTwoDesign(\n",
    "            initial_layer_weights=initial_layer_weights,\n",
    "            weights=weights,\n",
    "            wires=wires,\n",
    "        )\n",
    "\n",
    "def iqpe_reupload_su2_meas(\n",
    "    x, init_theta, theta, W, omega: float = 0.0\n",
    "):\n",
    "    iqpe_reupload_su2_parity(x, init_theta, theta, W, omega)\n",
    "    obs = parities(len(x))\n",
    "    H = qml.Hamiltonian(W, obs)\n",
    "\n",
    "    return qml.sample()\n",
    "\n",
    "#qnn_model = IQPEReuploadSU2Parity(params, omega=1.0)\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=num_qubits, shots=None)\n",
    "qnode = qml.QNode(iqpe_reupload_su2_meas, dev, interface=\"torch\")\n",
    "#drawer = qml.draw(qnode, expansion_strategy=\"device\")\n",
    "#print(drawer(x, init_theta, theta, W))\n",
    "probs = qnode(x, init_theta, theta, W, omega)\n",
    "probs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "samples = probs\n",
    "bitstrings = [''.join(str(b.item()) for b in sample) for sample in samples]\n",
    "bitstring_counts = {bs: bitstrings.count(bs) for bs in set(bitstrings)}\n",
    "print(samples)\n",
    "print(bitstrings)\n",
    "print(bitstring_counts)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from qml_mor.models import IQPEReuploadSU2Parity\n",
    "\n",
    "model = IQPEReuploadSU2Parity()\n",
    "\n",
    "qnode = qml.QNode(model.probabilities, dev, interface=\"torch\")\n",
    "probs = qnode(x, params)\n",
    "print(probs)\n",
    "print(model.Hamiltonian(params))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "H = model.Hamiltonian(params)\n",
    "sum = 0.0\n",
    "b = \"110\"\n",
    "\n",
    "for idx, O in enumerate(H.ops):\n",
    "\n",
    "    if not isinstance(O.name, list):\n",
    "        if O.name == \"Identity\":\n",
    "            sum += H.coeffs[idx]\n",
    "        elif O.name == \"PauliZ\":\n",
    "            i = O.wires[0]\n",
    "            sign = (-1)**(int(b[-1-i]))\n",
    "            sum += sign*H.coeffs[idx]\n",
    "        else:\n",
    "            raise ValueError(\"All operators must be PauliZ or Identity.\")\n",
    "    else:\n",
    "        if not all(name==\"PauliZ\" for name in O.name):\n",
    "            raise ValueError(\"All operators must be PauliZ or Identity.\")\n",
    "        \n",
    "        sign = 1\n",
    "        for w in O.wires:\n",
    "            sign *= (-1)**(int(b[-1-w]))\n",
    "\n",
    "        sum += sign*H.coeffs[idx]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(probs)\n",
    "marginal = qml.math.marginal_prob(probs, axis=[0])\n",
    "print(marginal)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from qml_mor.models import parities\n",
    "\n",
    "n = 3\n",
    "test = parities(n)\n",
    "H = qml.Hamiltonian(W, test)\n",
    "print(H)\n",
    "print(H.ops)"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# feature layer\n",
    "def feature_layer(x):\n",
    "    num_qubits = len(x)\n",
    "    qml.IQPEmbedding(x, wires=range(num_qubits))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# variational layer\n",
    "def variational_layer(init_theta, theta, num_qubits):\n",
    "    qml.SimplifiedTwoDesign(initial_layer_weights=init_theta, weights=theta, wires=range(num_qubits))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# observable / output layer\n",
    "def sequence_generator(n):\n",
    "    if n == 0:\n",
    "        return [[]]\n",
    "    else:\n",
    "        sequences = []\n",
    "        for sequence in sequence_generator(n-1):\n",
    "            sequences.append(sequence + [n-1])\n",
    "            sequences.append(sequence)\n",
    "        return sequences\n",
    "    \n",
    "def parities(n):\n",
    "    \n",
    "    seq = sequence_generator(n)\n",
    "    ops = []\n",
    "    for par in seq:\n",
    "        if par:\n",
    "            tmp = qml.PauliZ(par[0])\n",
    "            if len(par) > 1:\n",
    "                for i in par[1:]:\n",
    "                    tmp = tmp @ qml.PauliZ(i)\n",
    "\n",
    "            ops.append(tmp)\n",
    "\n",
    "    ops.append(qml.Identity(0))\n",
    "\n",
    "    return ops"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pennylane.templates import IQPEmbedding, SimplifiedTwoDesign\n",
    "# QNN model\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def qnn_model(x, init_theta, theta, W, omega=0.):\n",
    "\n",
    "    num_qubits = len(x)\n",
    "    shape_init = init_theta.shape\n",
    "    reps       = 1\n",
    "    if len(shape_init) < 1:\n",
    "        init_theta = [init_theta]\n",
    "        shape_init = init_theta.shape\n",
    "    reps       = shape_init[0]\n",
    "\n",
    "    shape_theta = theta.shape\n",
    "    reps_       = 1\n",
    "    if len(shape_theta) < 3:\n",
    "        theta       = [theta]\n",
    "        shape_theta = theta.shape\n",
    "    reps_       = shape_theta[0]\n",
    "\n",
    "    assert reps == reps_\n",
    "    for l in range(reps):\n",
    "        qml.IQPEmbedding(features=2**(omega*l)*x, wires=range(num_qubits))\n",
    "        qml.SimplifiedTwoDesign(initial_layer_weights=init_theta[l], weights=theta[l], wires=range(num_qubits))\n",
    "\n",
    "    obs = parities(num_qubits)\n",
    "    H   = qml.Hamiltonian(W, obs)\n",
    "\n",
    "    return qml.expval(H)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pennylane as qml\n",
    "from pennylane.templates import IQPEmbedding\n",
    "\n",
    "n_wires = 3\n",
    "n_layers = 2\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=n_wires)\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def iqpe_circuit(features):\n",
    "    for layer in range(n_layers):\n",
    "        # Apply the IQPEmbedding template in a loop\n",
    "        IQPEmbedding(features=features[layer], wires=range(n_wires))\n",
    "\n",
    "    return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_wires)]\n",
    "\n",
    "features = np.random.random((n_layers, n_wires))\n",
    "\n",
    "result1 = iqpe_circuit(features)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "num_qubits  = 3\n",
    "num_reps    = 0\n",
    "num_layers  = 0\n",
    "omega       = 1.\n",
    "x           = torch.randn(num_qubits, requires_grad=False)\n",
    "init_theta  = torch.randn(num_reps, num_qubits, requires_grad=True)\n",
    "theta       = torch.randn(num_reps, num_layers, num_qubits-1, 2, requires_grad=True)\n",
    "W           = torch.randn(2**num_qubits, requires_grad=True)\n",
    "\n",
    "result1 = qnn_model(x, init_theta, theta, W)\n",
    "result2 = qnn_model(x, init_theta, theta, W)\n",
    "\n",
    "ret = torch.stack([result1, result2])\n",
    "result1.size()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from scipy.stats import qmc\n",
    "def generate_samples_r(d, S):\n",
    "    sampler = qmc.LatinHypercube(d=d)\n",
    "    r_samples = sampler.random(n=S)\n",
    "    return r_samples\n",
    "r = generate_samples_r(10, 2)\n",
    "print(len(r[0]))\n",
    "\n",
    "tmp = set()\n",
    "rng = np.random.default_rng(seed=0)\n",
    "b1 = tuple(rng.integers(0, 2, size=4))\n",
    "b2 = tuple(rng.integers(0, 2, size=4))\n",
    "tmp.add(b1)\n",
    "tmp.add(b2)\n",
    "print(tmp)\n",
    "arr = np.array(list(tmp))\n",
    "print(arr)\n",
    "arr[0, 3]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "# Original list of tensors\n",
    "tensor_list = [torch.tensor([1., 2., 3.], requires_grad=True),\n",
    "               torch.tensor([4., 5., 6.], requires_grad=False)]\n",
    "\n",
    "est_params = [\n",
    "                        t.detach().clone().requires_grad_(t.requires_grad)\n",
    "                        for t in tensor_list\n",
    "                    ]\n",
    "tensor_list *= 2\n",
    "print(tensor_list)\n",
    "print(est_params)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "msg = (f\"Stopping early\\n\"\n",
    "       \"Loss not improving\")\n",
    "print(msg)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the quantum device\n",
    "dev = qml.device(\"default.qubit\", wires=2)\n",
    "\n",
    "# Define the custom model as a PennyLane QNode\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def custom_model(x, y):\n",
    "    # Apply your quantum circuit here, which uses input x and trainable parameters y\n",
    "    # As an example, we'll use a simple circuit with one rotation gate parameterized by y:\n",
    "    qml.RY(y[0], wires=0)\n",
    "    qml.RX(x, wires=1)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    return qml.expval(qml.PauliZ(1))\n",
    "\n",
    "# Define the dataset (x_data, y_data)\n",
    "x_data = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5], dtype=torch.float32)\n",
    "y_data = torch.tensor([0.5, 0.4, 0.3, 0.2, 0.1], dtype=torch.float32)\n",
    "\n",
    "# Initialize the trainable parameters\n",
    "params = torch.tensor([0.0], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Set the hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam([params], lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    predictions = torch.tensor([custom_model(x, params) for x in x_data], dtype=torch.float32)\n",
    "    loss = loss_fn(predictions, y_data)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "print(f\"Trained parameters: {params}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "num_qubits  = 3\n",
    "num_reps    = 3\n",
    "num_layers  = 0\n",
    "omega       = 1.\n",
    "x           = torch.randn(num_qubits, requires_grad=False)\n",
    "init_theta  = torch.randn(num_reps, num_qubits, requires_grad=True)\n",
    "theta       = torch.randn(num_reps, num_layers, num_qubits-1, 2, requires_grad=True)\n",
    "W           = torch.randn(2**num_qubits, requires_grad=True)\n",
    "def qnn_model(x, init_theta, theta, W, omega=0.):\n",
    "\n",
    "    num_qubits = len(x)\n",
    "    shape_init = init_theta.shape\n",
    "    reps       = 1\n",
    "    if len(shape_init) < 1:\n",
    "        init_theta = [init_theta]\n",
    "        shape_init = init_theta.shape\n",
    "    reps       = shape_init[0]\n",
    "\n",
    "    shape_theta = theta.shape\n",
    "    reps_       = 1\n",
    "    if len(shape_theta) < 3:\n",
    "        theta       = [theta]\n",
    "        shape_theta = theta.shape\n",
    "    reps_       = shape_theta[0]\n",
    "\n",
    "    assert reps == reps_\n",
    "    for l in range(reps):\n",
    "        qml.IQPEmbedding(features=2**(omega*l)*x, wires=range(num_qubits))\n",
    "        qml.SimplifiedTwoDesign(initial_layer_weights=init_theta[l], weights=theta[l], wires=range(num_qubits))\n",
    "\n",
    "    obs = parities(num_qubits)\n",
    "    H   = qml.Hamiltonian(W, obs)\n",
    "\n",
    "    return qml.expval(H)\n",
    "\n",
    "result = qnn_model(x, init_theta, theta, W)\n",
    "print(result.shape())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "tmp = parities(3)\n",
    "for x in tmp:\n",
    "    print(type(x))\n",
    "    print(issubclass(type(x), qml.operation.Observable))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# example\n",
    "num_qubits  = 3\n",
    "num_reps    = 3\n",
    "num_layers  = 0\n",
    "omega       = 1.\n",
    "x           = torch.randn(num_qubits, requires_grad=False)\n",
    "init_theta  = torch.randn(num_reps, num_qubits, requires_grad=True)\n",
    "theta       = torch.randn(num_reps, num_layers, num_qubits-1, 2, requires_grad=True)\n",
    "W           = torch.randn(2**num_qubits, requires_grad=True)\n",
    "\n",
    "print(qml.draw_mpl(qnn_model)(x, init_theta, theta, W, omega))"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets for Capacity Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def gen_dataset(N, samples=10, seed=0):\n",
    "    sizex   = num_qubits\n",
    "    scale   = 2.\n",
    "    shift   = -1.\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    x       = scale*torch.rand(N, sizex, requires_grad=False) + shift\n",
    "    y       = scale*torch.rand(samples, N, requires_grad=False) + shift\n",
    "\n",
    "    return x, y"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# model specs\n",
    "num_qubits  = 3\n",
    "num_reps    = 2\n",
    "num_layers  = 2\n",
    "omega       = 1."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import this"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# initial parameters\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "init_theta  = torch.randn(num_reps, num_qubits, requires_grad=True)\n",
    "theta       = torch.randn(num_reps, num_layers, num_qubits-1, 2, requires_grad=True)\n",
    "W           = torch.randn(2**num_qubits, requires_grad=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# loss function\n",
    "def square_loss(targets, predictions):\n",
    "    loss = 0\n",
    "    for t, p in zip(targets, predictions):\n",
    "        loss += (t - p) ** 2\n",
    "    loss = loss / len(targets)\n",
    "    return 0.5*loss"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# capacity estimation parameters\n",
    "Nmin     = 1\n",
    "Nmax     = 5\n",
    "samples  = 10\n",
    "steps    = 300\n",
    "eps_stop = 1e-12"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "summary = {}\n",
    "for N in range(Nmin, Nmax):\n",
    "    x, y = gen_dataset(N, samples)\n",
    "    \n",
    "    mre_sample = []\n",
    "    for s in range(samples):\n",
    "        print('===================================')\n",
    "        def cost(init_theta, theta, W):\n",
    "            pred = [qnn_model(x[k], init_theta, theta, W) for k in range(N)]\n",
    "            loss = square_loss(y[s], pred)\n",
    "            return loss\n",
    "        \n",
    "        # optimize\n",
    "        opt = torch.optim.Adam([init_theta, theta, W], lr=0.1, amsgrad=True)\n",
    "        for n in range(steps):\n",
    "            opt.zero_grad()\n",
    "            loss = cost(init_theta, theta, W)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            if n%10 == 9 or n == steps - 1:\n",
    "                print(f'{n+1}: {loss}')\n",
    "\n",
    "            if loss <= eps_stop:\n",
    "                break\n",
    "\n",
    "        # compute prediction errors\n",
    "        y_pred = torch.tensor([qnn_model(x[k], init_theta, theta, W) for k in range(N)], requires_grad=False)\n",
    "        mre    = torch.mean(torch.abs((y[s]-y_pred)/y_pred))\n",
    "        mre_sample.append(mre)\n",
    "\n",
    "    mre_N = torch.mean(torch.tensor(mre_sample))\n",
    "    summary[f'N = {N}'] = mre_N.item()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "for count, eps in enumerate(summary.values()):\n",
    "    m = int(np.log2(1./eps.item()))\n",
    "    C = (count+1)*m\n",
    "    print(C)\n",
    "print(torch.numel(init_theta))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "C = [-0.5, 13.1, 2]\n",
    "max(C)"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "y_pred = torch.tensor([qnn_model(x[k], init_theta, theta, W) for k in range(N)], requires_grad=False)\n",
    "mre    = torch.mean(torch.abs((y[s]-y_pred)/y_pred))\n",
    "print(mre.item())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# cutoff precision converted to bits of precision\n",
    "cutoff = np.sqrt(eps_stop)\n",
    "m      = np.log2(1./cutoff)\n",
    "print(m)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(x)\n",
    "print(init_theta)\n",
    "print(theta)\n",
    "print(W)\n",
    "print(y[s])\n",
    "print(y_pred)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QML-MOR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46e7eb4246c13550057ff7606884a772512028b1851bd0c4d2a0ba2e0c036465"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
