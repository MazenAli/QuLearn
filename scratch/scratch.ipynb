{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(level=logging.INFO)\n",
    "logger.info(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qulearn.qlayer import IQPEmbeddingLayer\n",
    "from qulearn.qlayer import QEvalType\n",
    "import torch\n",
    "import pennylane as qml\n",
    "from qulearn.observable import parity_all_hamiltonian \n",
    "\n",
    "numq = 3\n",
    "wires = range(numq)\n",
    "qdev = qml.device(\"lightning.qubit\", wires=numq, shots=None)\n",
    "@qml.qnode(qdev)\n",
    "def qnode():\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "out = qnode()\n",
    "print(out)\n",
    "print(qdev.shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "t = torch.nn.Parameter(torch.zeros(3, 3))\n",
    "print(t)\n",
    "torch.nn.init.uniform_(t, a=0.0, b=2*math.pi)\n",
    "print(t)\n",
    "a = [0, 1]\n",
    "b = [0, 2]\n",
    "print(a==b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2039],\n",
      "        [-0.2039],\n",
      "        [-0.2039]], dtype=torch.float64, grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from qulearn.qlayer import IQPEmbeddingLayer, RYCZLayer, MeasurementLayer, HamiltonianLayer, MeasurementType, EmbedVarLayer, CircuitLayer\n",
    "import pennylane as qml\n",
    "import torch\n",
    "from qulearn.observable import parities_all_observables\n",
    "\n",
    "observable = qml.PauliZ(0)\n",
    "num_wires = 3\n",
    "num_reup = 1\n",
    "num_layers = 1\n",
    "num_repeat = 3\n",
    "qdev = qml.device('default.qubit.autograd', wires=num_wires, shots=None)\n",
    "\n",
    "upload_layer = IQPEmbeddingLayer(num_wires, num_reup)\n",
    "var_layer = RYCZLayer(num_wires, num_layers)\n",
    "embedvar = EmbedVarLayer(upload_layer, var_layer, n_repeat=num_repeat, omega=1.0)\n",
    "measure_layer = MeasurementLayer(embedvar, qdevice=qdev, measurement_type=MeasurementType.Expectation, observable=observable)\n",
    "obs = parities_all_observables(num_wires)\n",
    "ham_layer = HamiltonianLayer(embedvar,  observables=obs, interface='torch', diff_method='backprop')\n",
    "#ham_layer = HamiltonianLayer(upload_layer, observables=obs)\n",
    "\n",
    "x = torch.zeros(3, num_wires)\n",
    "print(ham_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.2721,  0.4321, -0.1586, -0.6842, -0.1647, -0.7057,  1.1033, -1.0298],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([3.8389, 1.4313, 3.5868], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[1.1019, 0.7775],\n",
      "         [2.8750, 2.0863]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2721,  0.4321, -0.1586, -0.6842, -0.1647, -0.7057,  1.1033, -1.0298],\n",
      "       requires_grad=True)\n",
      "  (-1.0297938585281372) [I0]\n",
      "+ (-0.70566725730896) [Z1]\n",
      "+ (-0.684208869934082) [Z0]\n",
      "+ (1.103307843208313) [Z2]\n",
      "+ (-0.16468751430511475) [Z1 Z2]\n",
      "+ (-0.15856914222240448) [Z0 Z2]\n",
      "+ (0.4320738613605499) [Z0 Z1]\n",
      "+ (-0.27214688062667847) [Z0 Z1 Z2]\n"
     ]
    }
   ],
   "source": [
    "for p in ham_layer.parameters():\n",
    "    print(p)\n",
    "print(ham_layer.observable.coeffs)\n",
    "print(ham_layer.observable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = torch.tensor([1., -1., 0.5])\n",
    "print(tmp.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qulearn.fim import empirical_fim, compute_fims, mc_integrate_idfim_det\n",
    "\n",
    "FIM = empirical_fim(measure_layer, x)\n",
    "for p in measure_layer.parameters():\n",
    "    print(p)\n",
    "\n",
    "plist = []\n",
    "p1 = torch.randn(4, 3)\n",
    "p2 = torch.randn(4, 3, 2, 2)\n",
    "for i in range(4):\n",
    "    plist.append([p1[i], p2[i]])\n",
    "\n",
    "FIMs = compute_fims(measure_layer, x, plist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integral = mc_integrate_idfim_det(FIMs, 1.0, 1.0)\n",
    "print(integral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = torch.nn.Linear(3, 1)\n",
    "print(lin(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "QuantumFunctionError",
     "evalue": "Adjoint differentiation method does not support Hamiltonian observables.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mQuantumFunctionError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m y2 \u001b[39m=\u001b[39m var_layer(x)\n\u001b[1;32m      3\u001b[0m \u001b[39m#y3 = measure_layer(x)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m y4 \u001b[39m=\u001b[39m ham_layer(x)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(y3)\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(y4)\n",
      "File \u001b[0;32m/home/software/anaconda3/envs/QuLearn/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Research/QC/QuLearn/qulearn/qlayer.py:142\u001b[0m, in \u001b[0;36mMeasurementLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    140\u001b[0m         out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munsqueeze(out, \u001b[39m0\u001b[39m)\n\u001b[1;32m    141\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     outs \u001b[39m=\u001b[39m [qnode(xk) \u001b[39mfor\u001b[39;49;00m xk \u001b[39min\u001b[39;49;00m torch\u001b[39m.\u001b[39;49munbind(x)]\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeasurement_type \u001b[39m==\u001b[39m MeasurementType\u001b[39m.\u001b[39mExpectation:\n\u001b[1;32m    144\u001b[0m         outs \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39munsqueeze(out, \u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m out \u001b[39min\u001b[39;00m outs]\n",
      "File \u001b[0;32m~/Research/QC/QuLearn/qulearn/qlayer.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    140\u001b[0m         out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munsqueeze(out, \u001b[39m0\u001b[39m)\n\u001b[1;32m    141\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     outs \u001b[39m=\u001b[39m [qnode(xk) \u001b[39mfor\u001b[39;00m xk \u001b[39min\u001b[39;00m torch\u001b[39m.\u001b[39munbind(x)]\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeasurement_type \u001b[39m==\u001b[39m MeasurementType\u001b[39m.\u001b[39mExpectation:\n\u001b[1;32m    144\u001b[0m         outs \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39munsqueeze(out, \u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m out \u001b[39min\u001b[39;00m outs]\n",
      "File \u001b[0;32m/home/software/anaconda3/envs/QuLearn/lib/python3.11/site-packages/pennylane/qnode.py:847\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_original_device()\n\u001b[1;32m    845\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[0;32m--> 847\u001b[0m res \u001b[39m=\u001b[39m qml\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    848\u001b[0m     [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtape],\n\u001b[1;32m    849\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice,\n\u001b[1;32m    850\u001b[0m     gradient_fn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient_fn,\n\u001b[1;32m    851\u001b[0m     interface\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterface,\n\u001b[1;32m    852\u001b[0m     gradient_kwargs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient_kwargs,\n\u001b[1;32m    853\u001b[0m     override_shots\u001b[39m=\u001b[39;49moverride_shots,\n\u001b[1;32m    854\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute_kwargs,\n\u001b[1;32m    855\u001b[0m )\n\u001b[1;32m    857\u001b[0m \u001b[39mif\u001b[39;00m old_interface \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    858\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterface \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/home/software/anaconda3/envs/QuLearn/lib/python3.11/site-packages/pennylane/interfaces/execution.py:724\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(tapes, device, gradient_fn, interface, mode, gradient_kwargs, cache, cachesize, max_diff, override_shots, expand_fn, max_expansion, device_batch_transform)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    719\u001b[0m     \u001b[39mraise\u001b[39;00m qml\u001b[39m.\u001b[39mQuantumFunctionError(\n\u001b[1;32m    720\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmapped_interface\u001b[39m}\u001b[39;00m\u001b[39m not found. Please install the latest \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    721\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mversion of \u001b[39m\u001b[39m{\u001b[39;00mmapped_interface\u001b[39m}\u001b[39;00m\u001b[39m to enable the \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmapped_interface\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m interface.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    722\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m--> 724\u001b[0m res \u001b[39m=\u001b[39m _execute(\n\u001b[1;32m    725\u001b[0m     tapes, device, execute_fn, gradient_fn, gradient_kwargs, _n\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, max_diff\u001b[39m=\u001b[39;49mmax_diff, mode\u001b[39m=\u001b[39;49m_mode\n\u001b[1;32m    726\u001b[0m )\n\u001b[1;32m    728\u001b[0m \u001b[39mreturn\u001b[39;00m batch_fn(res)\n",
      "File \u001b[0;32m/home/software/anaconda3/envs/QuLearn/lib/python3.11/site-packages/pennylane/interfaces/torch.py:258\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(tapes, device, execute_fn, gradient_fn, gradient_kwargs, _n, max_diff, mode)\u001b[0m\n\u001b[1;32m    247\u001b[0m     parameters\u001b[39m.\u001b[39mextend(tape\u001b[39m.\u001b[39mget_parameters())\n\u001b[1;32m    249\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    250\u001b[0m     tapes\u001b[39m=\u001b[39mtapes,\n\u001b[1;32m    251\u001b[0m     device\u001b[39m=\u001b[39mdevice,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    256\u001b[0m     max_diff\u001b[39m=\u001b[39mmax_diff,\n\u001b[1;32m    257\u001b[0m )\n\u001b[0;32m--> 258\u001b[0m \u001b[39mreturn\u001b[39;00m ExecuteTapes\u001b[39m.\u001b[39;49mapply(kwargs, \u001b[39m*\u001b[39;49mparameters)\n",
      "File \u001b[0;32m/home/software/anaconda3/envs/QuLearn/lib/python3.11/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/home/software/anaconda3/envs/QuLearn/lib/python3.11/site-packages/pennylane/interfaces/torch.py:87\u001b[0m, in \u001b[0;36mExecuteTapes.forward\u001b[0;34m(ctx, kwargs, *parameters)\u001b[0m\n\u001b[1;32m     84\u001b[0m ctx\u001b[39m.\u001b[39m_n \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_n\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[39mwith\u001b[39;00m qml\u001b[39m.\u001b[39mtape\u001b[39m.\u001b[39mUnwrap(\u001b[39m*\u001b[39mctx\u001b[39m.\u001b[39mtapes):\n\u001b[0;32m---> 87\u001b[0m     res, ctx\u001b[39m.\u001b[39mjacs \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39;49mexecute_fn(ctx\u001b[39m.\u001b[39;49mtapes, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mctx\u001b[39m.\u001b[39;49mgradient_kwargs)\n\u001b[1;32m     89\u001b[0m \u001b[39m# if any input tensor uses the GPU, the output should as well\u001b[39;00m\n\u001b[1;32m     90\u001b[0m ctx\u001b[39m.\u001b[39mtorch_device \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/software/anaconda3/envs/QuLearn/lib/python3.11/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m/home/software/anaconda3/envs/QuLearn/lib/python3.11/site-packages/pennylane/_device.py:575\u001b[0m, in \u001b[0;36mDevice.execute_and_gradients\u001b[0;34m(self, circuits, method, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[39mfor\u001b[39;00m circuit \u001b[39min\u001b[39;00m circuits:\n\u001b[1;32m    571\u001b[0m     \u001b[39m# Evaluations and gradients are paired, so that\u001b[39;00m\n\u001b[1;32m    572\u001b[0m     \u001b[39m# devices can re-use the device state for the\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     \u001b[39m# gradient computation (if applicable).\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     res\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_execute([circuit])[\u001b[39m0\u001b[39m])\n\u001b[0;32m--> 575\u001b[0m     jacs\u001b[39m.\u001b[39mappend(gradient_method(circuit, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    577\u001b[0m \u001b[39mreturn\u001b[39;00m res, jacs\n",
      "File \u001b[0;32m/home/software/anaconda3/envs/QuLearn/lib/python3.11/site-packages/pennylane/_qubit_device.py:1985\u001b[0m, in \u001b[0;36mQubitDevice.adjoint_jacobian\u001b[0;34m(self, tape, starting_state, use_device_state)\u001b[0m\n\u001b[1;32m   1979\u001b[0m     \u001b[39mraise\u001b[39;00m qml\u001b[39m.\u001b[39mQuantumFunctionError(\n\u001b[1;32m   1980\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAdjoint differentiation method does not support\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1981\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m measurement \u001b[39m\u001b[39m{\u001b[39;00mm\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1982\u001b[0m     )\n\u001b[1;32m   1984\u001b[0m \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mobs\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHamiltonian\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 1985\u001b[0m     \u001b[39mraise\u001b[39;00m qml\u001b[39m.\u001b[39mQuantumFunctionError(\n\u001b[1;32m   1986\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAdjoint differentiation method does not support Hamiltonian observables.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1987\u001b[0m     )\n\u001b[1;32m   1989\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(m\u001b[39m.\u001b[39mobs, \u001b[39m\"\u001b[39m\u001b[39mbase_name\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1990\u001b[0m     m\u001b[39m.\u001b[39mobs\u001b[39m.\u001b[39mbase_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# This is needed for when the observable is a tensor product\u001b[39;00m\n",
      "\u001b[0;31mQuantumFunctionError\u001b[0m: Adjoint differentiation method does not support Hamiltonian observables."
     ]
    }
   ],
   "source": [
    "y1 = upload_layer(x)\n",
    "y2 = var_layer(x)\n",
    "#y3 = measure_layer(x)\n",
    "y4 = ham_layer(x)\n",
    "\n",
    "print(y3)\n",
    "print(y4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(y3)\n",
    "log_prob = torch.log(y3)\n",
    "print(torch.log(y3))\n",
    "print(x.shape)\n",
    "\n",
    "measure_layer.zero_grad()\n",
    "log_prob.backward(retain_graph=True)\n",
    "grad_list = [\n",
    "    p.grad.view(-1)\n",
    "    for p in measure_layer.parameters()\n",
    "    if p.requires_grad and p.grad is not None\n",
    "]\n",
    "grad = torch.cat(grad_list)\n",
    "prod = torch.outer(grad, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in measure_layer.state_dict().items():\n",
    "    print(key)\n",
    "    print(val)\n",
    "    print(\"====\")\n",
    "\n",
    "print(\"************************************\")\n",
    "\n",
    "for key, val in ham_layer.state_dict().items():\n",
    "    print(key)\n",
    "    print(val)\n",
    "    print(\"====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ll_in = nn.Linear(3, 3, dtype=torch.float64)\n",
    "        self.meas = ham_layer\n",
    "        self.ll_out = nn.Linear(1, 1, dtype=torch.float64)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        y = self.ll_in(x)\n",
    "        y = self.meas(y)\n",
    "        y = self.ll_out(y)\n",
    "\n",
    "        return y\n",
    "    \n",
    "hybrid = HybridModel()\n",
    "x = torch.rand(3, dtype=torch.float64)\n",
    "y = hybrid(x)\n",
    "print(y)\n",
    "for key, val in hybrid.state_dict().items():\n",
    "    print(key)\n",
    "    print(val)\n",
    "    print(\"======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in measure_layer.state_dict().items():\n",
    "    print(key)\n",
    "    print(val)\n",
    "    print(\"=====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class MyType(Enum):\n",
    "\n",
    "    TypeA = 0\n",
    "    TypeB = 1\n",
    "\n",
    "x = MyType.TypeA\n",
    "y = \"blub\"\n",
    "\n",
    "if x == MyType.TypeA:\n",
    "    print(x)\n",
    "\n",
    "if not isinstance(y, MyType):\n",
    "    raise NotImplementedError(\"QEvalType not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "model = Linear(3, 1)\n",
    "X = torch.tensor([[0.1, 1.1, -2.2], [0.6, 4.1, -3.2], [-0.1, -2.1, -2.2]])\n",
    "Y = model(X)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"runs/test\")\n",
    "for i in range(10):\n",
    "    val = 1e-01\n",
    "    writer.add_scalar(\"foobar1\", val, i)\n",
    "    writer.add_scalars(\"loss\", {\"train\": val, \"valid\": val+1.0}, i)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pennylane as qml\n",
    "from qml_mor.models import IQPEReuploadSU2Parity, ModelType\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "num_samples=1\n",
    "num_qubits = 3\n",
    "num_reups = 1\n",
    "num_layers = 1\n",
    "sizex = num_qubits\n",
    "\n",
    "omega = 0.0\n",
    "init_theta = torch.randn(num_reups, num_qubits, requires_grad=True)\n",
    "theta = torch.randn(\n",
    "    num_reups, num_layers, num_qubits - 1, 2, requires_grad=True\n",
    ")\n",
    "W = torch.randn(2**num_qubits, requires_grad=True)\n",
    "\n",
    "params = [init_theta, theta, W]\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=num_qubits, shots=None)\n",
    "\n",
    "model = IQPEReuploadSU2Parity(dev, params, model_type=ModelType.Expectation)\n",
    "X = torch.randn(10, 3)\n",
    "Ypred = model(X)\n",
    "print(Ypred)\n",
    "print(Ypred.shape)\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "test = loss(Ypred, Ypred)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Parameter\n",
    "P = model.parameters()\n",
    "for p in P:\n",
    "    print(p)\n",
    "print(\"====================\")\n",
    "save = model.state_dict().copy()\n",
    "\n",
    "init_theta = torch.randn(num_reups, num_qubits, requires_grad=True)\n",
    "theta = torch.randn(\n",
    "    num_reups, num_layers, num_qubits - 1, 2, requires_grad=True\n",
    ")\n",
    "W = torch.randn(2**num_qubits, requires_grad=True)\n",
    "\n",
    "params = [init_theta, theta, W]\n",
    "\n",
    "model = IQPEReuploadSU2Parity(dev, params)\n",
    "\n",
    "P = model.parameters()\n",
    "for p in P:\n",
    "    print(p)\n",
    "print(\"====================\")\n",
    "model.load_state_dict(save)\n",
    "P = model.parameters()\n",
    "for p in P:\n",
    "    print(p)\n",
    "print(\"====================\")\n",
    "print(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "QuantumFunctionError",
     "evalue": "Adjoint differentiation method does not support Hamiltonian observables.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mQuantumFunctionError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m ham_layer \u001b[39m=\u001b[39m HamiltonianLayer(embedvar,  qdevice\u001b[39m=\u001b[39mqdev, observables\u001b[39m=\u001b[39mobs, interface\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m'\u001b[39m, diff_method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madjoint\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39m3\u001b[39m, num_wires)\n\u001b[0;32m---> 22\u001b[0m \u001b[39mprint\u001b[39m(ham_layer(x))\n",
      "File \u001b[0;32m/home/software/anaconda3/envs/QuLearn/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Research/QC/QuLearn/qulearn/qlayer.py:142\u001b[0m, in \u001b[0;36mMeasurementLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    140\u001b[0m         out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munsqueeze(out, \u001b[39m0\u001b[39m)\n\u001b[1;32m    141\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     outs \u001b[39m=\u001b[39m [qnode(xk) \u001b[39mfor\u001b[39;49;00m xk \u001b[39min\u001b[39;49;00m torch\u001b[39m.\u001b[39;49munbind(x)]\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeasurement_type \u001b[39m==\u001b[39m MeasurementType\u001b[39m.\u001b[39mExpectation:\n\u001b[1;32m    144\u001b[0m         outs \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39munsqueeze(out, \u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m out \u001b[39min\u001b[39;00m outs]\n",
      "File \u001b[0;32m~/Research/QC/QuLearn/qulearn/qlayer.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    140\u001b[0m         out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munsqueeze(out, \u001b[39m0\u001b[39m)\n\u001b[1;32m    141\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     outs \u001b[39m=\u001b[39m [qnode(xk) \u001b[39mfor\u001b[39;00m xk \u001b[39min\u001b[39;00m torch\u001b[39m.\u001b[39munbind(x)]\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeasurement_type \u001b[39m==\u001b[39m MeasurementType\u001b[39m.\u001b[39mExpectation:\n\u001b[1;32m    144\u001b[0m         outs \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39munsqueeze(out, \u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m out \u001b[39min\u001b[39;00m outs]\n",
      "File \u001b[0;32m/home/software/anaconda3/envs/QuLearn/lib/python3.11/site-packages/pennylane/qnode.py:847\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_original_device()\n\u001b[1;32m    845\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[0;32m--> 847\u001b[0m res \u001b[39m=\u001b[39m qml\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    848\u001b[0m     [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtape],\n\u001b[1;32m    849\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice,\n\u001b[1;32m    850\u001b[0m     gradient_fn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient_fn,\n\u001b[1;32m    851\u001b[0m     interface\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterface,\n\u001b[1;32m    852\u001b[0m     gradient_kwargs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient_kwargs,\n\u001b[1;32m    853\u001b[0m     override_shots\u001b[39m=\u001b[39;49moverride_shots,\n\u001b[1;32m    854\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute_kwargs,\n\u001b[1;32m    855\u001b[0m )\n\u001b[1;32m    857\u001b[0m \u001b[39mif\u001b[39;00m old_interface \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    858\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterface \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/home/software/anaconda3/envs/QuLearn/lib/python3.11/site-packages/pennylane/interfaces/execution.py:724\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(tapes, device, gradient_fn, interface, mode, gradient_kwargs, cache, cachesize, max_diff, override_shots, expand_fn, max_expansion, device_batch_transform)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    719\u001b[0m     \u001b[39mraise\u001b[39;00m qml\u001b[39m.\u001b[39mQuantumFunctionError(\n\u001b[1;32m    720\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmapped_interface\u001b[39m}\u001b[39;00m\u001b[39m not found. Please install the latest \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    721\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mversion of \u001b[39m\u001b[39m{\u001b[39;00mmapped_interface\u001b[39m}\u001b[39;00m\u001b[39m to enable the \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmapped_interface\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m interface.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    722\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m--> 724\u001b[0m res \u001b[39m=\u001b[39m _execute(\n\u001b[1;32m    725\u001b[0m     tapes, device, execute_fn, gradient_fn, gradient_kwargs, _n\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, max_diff\u001b[39m=\u001b[39;49mmax_diff, mode\u001b[39m=\u001b[39;49m_mode\n\u001b[1;32m    726\u001b[0m )\n\u001b[1;32m    728\u001b[0m \u001b[39mreturn\u001b[39;00m batch_fn(res)\n",
      "File \u001b[0;32m/home/software/anaconda3/envs/QuLearn/lib/python3.11/site-packages/pennylane/interfaces/torch.py:258\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(tapes, device, execute_fn, gradient_fn, gradient_kwargs, _n, max_diff, mode)\u001b[0m\n\u001b[1;32m    247\u001b[0m     parameters\u001b[39m.\u001b[39mextend(tape\u001b[39m.\u001b[39mget_parameters())\n\u001b[1;32m    249\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    250\u001b[0m     tapes\u001b[39m=\u001b[39mtapes,\n\u001b[1;32m    251\u001b[0m     device\u001b[39m=\u001b[39mdevice,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    256\u001b[0m     max_diff\u001b[39m=\u001b[39mmax_diff,\n\u001b[1;32m    257\u001b[0m )\n\u001b[0;32m--> 258\u001b[0m \u001b[39mreturn\u001b[39;00m ExecuteTapes\u001b[39m.\u001b[39;49mapply(kwargs, \u001b[39m*\u001b[39;49mparameters)\n",
      "File \u001b[0;32m/home/software/anaconda3/envs/QuLearn/lib/python3.11/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/home/software/anaconda3/envs/QuLearn/lib/python3.11/site-packages/pennylane/interfaces/torch.py:87\u001b[0m, in \u001b[0;36mExecuteTapes.forward\u001b[0;34m(ctx, kwargs, *parameters)\u001b[0m\n\u001b[1;32m     84\u001b[0m ctx\u001b[39m.\u001b[39m_n \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_n\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[39mwith\u001b[39;00m qml\u001b[39m.\u001b[39mtape\u001b[39m.\u001b[39mUnwrap(\u001b[39m*\u001b[39mctx\u001b[39m.\u001b[39mtapes):\n\u001b[0;32m---> 87\u001b[0m     res, ctx\u001b[39m.\u001b[39mjacs \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39;49mexecute_fn(ctx\u001b[39m.\u001b[39;49mtapes, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mctx\u001b[39m.\u001b[39;49mgradient_kwargs)\n\u001b[1;32m     89\u001b[0m \u001b[39m# if any input tensor uses the GPU, the output should as well\u001b[39;00m\n\u001b[1;32m     90\u001b[0m ctx\u001b[39m.\u001b[39mtorch_device \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/software/anaconda3/envs/QuLearn/lib/python3.11/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m/home/software/anaconda3/envs/QuLearn/lib/python3.11/site-packages/pennylane/_device.py:575\u001b[0m, in \u001b[0;36mDevice.execute_and_gradients\u001b[0;34m(self, circuits, method, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[39mfor\u001b[39;00m circuit \u001b[39min\u001b[39;00m circuits:\n\u001b[1;32m    571\u001b[0m     \u001b[39m# Evaluations and gradients are paired, so that\u001b[39;00m\n\u001b[1;32m    572\u001b[0m     \u001b[39m# devices can re-use the device state for the\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     \u001b[39m# gradient computation (if applicable).\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     res\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_execute([circuit])[\u001b[39m0\u001b[39m])\n\u001b[0;32m--> 575\u001b[0m     jacs\u001b[39m.\u001b[39mappend(gradient_method(circuit, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    577\u001b[0m \u001b[39mreturn\u001b[39;00m res, jacs\n",
      "File \u001b[0;32m/home/software/anaconda3/envs/QuLearn/lib/python3.11/site-packages/pennylane/_qubit_device.py:1985\u001b[0m, in \u001b[0;36mQubitDevice.adjoint_jacobian\u001b[0;34m(self, tape, starting_state, use_device_state)\u001b[0m\n\u001b[1;32m   1979\u001b[0m     \u001b[39mraise\u001b[39;00m qml\u001b[39m.\u001b[39mQuantumFunctionError(\n\u001b[1;32m   1980\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAdjoint differentiation method does not support\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1981\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m measurement \u001b[39m\u001b[39m{\u001b[39;00mm\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1982\u001b[0m     )\n\u001b[1;32m   1984\u001b[0m \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mobs\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHamiltonian\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 1985\u001b[0m     \u001b[39mraise\u001b[39;00m qml\u001b[39m.\u001b[39mQuantumFunctionError(\n\u001b[1;32m   1986\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAdjoint differentiation method does not support Hamiltonian observables.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1987\u001b[0m     )\n\u001b[1;32m   1989\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(m\u001b[39m.\u001b[39mobs, \u001b[39m\"\u001b[39m\u001b[39mbase_name\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1990\u001b[0m     m\u001b[39m.\u001b[39mobs\u001b[39m.\u001b[39mbase_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# This is needed for when the observable is a tensor product\u001b[39;00m\n",
      "\u001b[0;31mQuantumFunctionError\u001b[0m: Adjoint differentiation method does not support Hamiltonian observables."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "from qulearn.qlayer import IQPEmbeddingLayer, RYCZLayer, MeasurementLayer, HamiltonianLayer, MeasurementType, EmbedVarLayer, CircuitLayer\n",
    "import pennylane as qml\n",
    "import torch\n",
    "from qulearn.observable import parities_all_observables\n",
    "\n",
    "observable = qml.PauliZ(0)\n",
    "num_wires = 3\n",
    "num_reup = 1\n",
    "num_layers = 1\n",
    "num_repeat = 3\n",
    "qdev = qml.device('default.qubit', wires=num_wires, shots=None)\n",
    "\n",
    "upload_layer = IQPEmbeddingLayer(num_wires, num_reup)\n",
    "var_layer = RYCZLayer(num_wires, num_layers)\n",
    "embedvar = EmbedVarLayer(upload_layer, var_layer, n_repeat=num_repeat, omega=1.0)\n",
    "obs = parities_all_observables(num_wires)\n",
    "ham_layer = HamiltonianLayer(embedvar,  qdevice=qdev, observables=obs, interface='torch', diff_method='adjoint')\n",
    "\n",
    "x = torch.zeros(3, num_wires)\n",
    "print(ham_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "N = 100\n",
    "torch.manual_seed(0)\n",
    "X = torch.randn(N, 10, dtype=torch.float64)\n",
    "#Y = torch.randn(N, 1, dtype=torch.float64)\n",
    "A = torch.randn(10, 1, dtype=torch.float64)\n",
    "b = torch.randn(1, dtype=torch.float64)\n",
    "eps = torch.randn(N, dtype=torch.float64)*0.001\n",
    "Y = torch.matmul(X, A) + b + eps\n",
    "model = torch.nn.Linear(10, 1, bias=True, dtype=torch.float64)\n",
    "batch_size=4\n",
    "shuffle=True\n",
    "\n",
    "dataset = TensorDataset(X, Y)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from qulearn.trainer import SupervisedTrainer\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "opt = Adam(model.parameters(), lr=0.1, amsgrad=True)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "metrics = {\"L1\": loss_fn, \"L2\": loss_fn}\n",
    "writer = SummaryWriter()\n",
    "logger = logging.getLogger(\"SupTrainer\")\n",
    "logger.setLevel(level=logging.INFO)\n",
    "trainer = SupervisedTrainer(opt, loss_fn, metrics, 200, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:SupTrainer:Train - Epoch: 1, Loss: 0.5244, Metrics: L1: 0.5244, L2: 0.5244\n",
      "INFO:SupTrainer:Validate - Epoch: 1, Loss: 0.5244, Metrics: L1: 0.5244, L2: 0.5244\n",
      "INFO:SupTrainer:Train - Epoch: 2, Loss: 0.1121, Metrics: L1: 0.1121, L2: 0.1121\n",
      "INFO:SupTrainer:Validate - Epoch: 2, Loss: 0.1121, Metrics: L1: 0.1121, L2: 0.1121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:SupTrainer:Train - Epoch: 3, Loss: 0.0101, Metrics: L1: 0.0101, L2: 0.0101\n",
      "INFO:SupTrainer:Validate - Epoch: 3, Loss: 0.0101, Metrics: L1: 0.0101, L2: 0.0101\n",
      "INFO:SupTrainer:Train - Epoch: 4, Loss: 0.0007, Metrics: L1: 0.0007, L2: 0.0007\n",
      "INFO:SupTrainer:Validate - Epoch: 4, Loss: 0.0007, Metrics: L1: 0.0007, L2: 0.0007\n",
      "INFO:SupTrainer:Train - Epoch: 5, Loss: 0.0001, Metrics: L1: 0.0001, L2: 0.0001\n",
      "INFO:SupTrainer:Validate - Epoch: 5, Loss: 0.0001, Metrics: L1: 0.0001, L2: 0.0001\n",
      "INFO:SupTrainer:Train - Epoch: 6, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 6, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 7, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 7, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 8, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 8, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 9, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 9, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 10, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 10, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 11, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 11, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 12, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 12, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 13, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 13, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 14, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 14, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 15, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 15, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 16, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 16, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 17, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 17, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 18, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 18, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 19, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 19, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 20, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 20, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 21, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 21, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 22, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 22, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 23, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 23, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 24, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 24, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 25, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 25, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 26, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 26, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 27, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 27, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 28, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 28, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 29, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 29, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 30, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 30, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 31, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 31, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 32, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 32, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 33, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 33, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 34, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 34, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 35, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 35, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 36, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 36, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 37, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 37, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 38, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 38, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 39, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 39, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 40, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 40, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 41, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 41, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 42, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 42, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 43, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 43, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 44, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 44, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 45, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 45, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 46, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 46, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 47, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 47, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 48, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 48, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 49, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 49, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 50, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 50, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 51, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 51, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 52, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 52, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 53, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 53, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 54, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 54, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 55, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 55, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 56, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 56, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 57, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 57, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 58, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 58, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 59, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 59, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 60, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 60, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 61, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 61, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 62, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 62, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 63, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 63, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 64, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 64, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 65, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 65, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 66, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 66, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 67, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 67, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 68, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 68, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 69, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 69, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 70, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 70, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 71, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 71, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 72, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 72, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 73, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 73, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 74, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 74, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 75, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 75, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 76, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 76, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 77, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 77, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 78, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 78, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 79, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 79, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 80, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 80, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 81, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 81, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 82, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 82, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 83, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 83, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 84, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 84, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 85, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 85, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 86, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 86, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 87, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 87, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 88, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 88, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 89, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 89, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 90, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 90, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 91, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 91, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 92, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 92, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 93, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 93, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 94, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 94, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 95, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 95, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 96, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 96, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 97, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 97, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 98, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 98, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 99, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 99, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 100, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 100, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 101, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 101, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 102, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 102, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 103, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 103, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 104, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 104, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 105, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 105, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 106, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 106, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 107, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 107, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 108, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 108, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 109, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 109, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 110, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 110, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 111, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 111, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 112, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 112, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 113, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 113, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 114, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 114, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 115, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 115, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 116, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 116, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 117, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 117, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 118, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 118, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 119, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 119, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 120, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 120, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 121, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 121, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 122, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 122, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 123, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 123, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 124, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 124, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 125, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 125, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 126, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 126, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 127, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 127, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 128, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 128, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 129, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 129, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 130, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 130, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 131, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 131, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 132, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 132, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 133, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 133, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 134, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 134, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 135, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 135, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 136, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 136, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 137, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 137, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 138, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 138, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 139, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 139, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 140, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 140, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 141, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 141, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 142, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 142, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 143, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 143, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 144, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 144, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 145, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 145, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 146, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 146, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 147, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 147, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 148, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 148, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 149, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 149, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 150, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 150, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 151, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 151, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 152, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 152, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 153, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 153, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 154, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 154, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 155, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 155, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 156, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 156, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 157, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 157, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 158, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 158, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 159, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 159, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 160, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 160, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 161, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 161, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 162, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 162, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 163, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 163, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 164, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 164, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 165, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 165, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 166, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 166, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 167, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 167, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 168, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 168, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 169, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 169, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 170, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 170, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 171, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 171, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 172, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 172, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 173, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 173, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 174, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 174, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 175, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 175, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 176, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 176, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 177, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 177, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 178, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 178, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 179, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 179, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 180, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 180, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 181, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 181, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 182, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 182, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 183, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 183, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 184, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 184, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 185, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 185, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 186, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 186, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 187, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 187, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 188, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 188, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 189, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 189, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 190, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 190, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 191, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 191, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 192, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 192, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 193, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 193, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 194, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 194, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 195, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 195, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 196, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 196, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 197, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 197, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 198, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 198, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 199, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 199, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Train - Epoch: 200, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n",
      "INFO:SupTrainer:Validate - Epoch: 200, Loss: 0.0000, Metrics: L1: 0.0000, L2: 0.0000\n"
     ]
    }
   ],
   "source": [
    "trainer.train(model, loader, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:SupTrainer:Train - Epoch: 1, Loss: 1.9839, Metrics: L1: 1.9839, L2: 1.9839\n",
      "INFO:SupTrainer:Validate - Epoch: 1, Loss: 1.9839, Metrics: L1: 1.9839, L2: 1.9839\n",
      "INFO:SupTrainer:Train - Epoch: 2, Loss: 1.3793, Metrics: L1: 1.3793, L2: 1.3793\n",
      "INFO:SupTrainer:Validate - Epoch: 2, Loss: 1.3793, Metrics: L1: 1.3793, L2: 1.3793\n",
      "INFO:SupTrainer:Train - Epoch: 3, Loss: 1.6594, Metrics: L1: 1.6594, L2: 1.6594\n",
      "INFO:SupTrainer:Validate - Epoch: 3, Loss: 1.6594, Metrics: L1: 1.6594, L2: 1.6594\n",
      "INFO:SupTrainer:Train - Epoch: 4, Loss: 1.7433, Metrics: L1: 1.7433, L2: 1.7433\n",
      "INFO:SupTrainer:Validate - Epoch: 4, Loss: 1.7433, Metrics: L1: 1.7433, L2: 1.7433\n",
      "INFO:SupTrainer:Train - Epoch: 5, Loss: 2.0461, Metrics: L1: 2.0461, L2: 2.0461\n",
      "INFO:SupTrainer:Validate - Epoch: 5, Loss: 2.0461, Metrics: L1: 2.0461, L2: 2.0461\n",
      "INFO:SupTrainer:Train - Epoch: 1, Loss: 1.5082, Metrics: L1: 1.5082, L2: 1.5082\n",
      "INFO:SupTrainer:Validate - Epoch: 1, Loss: 1.5082, Metrics: L1: 1.5082, L2: 1.5082\n",
      "INFO:SupTrainer:Train - Epoch: 2, Loss: 1.5672, Metrics: L1: 1.5672, L2: 1.5672\n",
      "INFO:SupTrainer:Validate - Epoch: 2, Loss: 1.5672, Metrics: L1: 1.5672, L2: 1.5672\n",
      "INFO:SupTrainer:Train - Epoch: 3, Loss: 1.6736, Metrics: L1: 1.6736, L2: 1.6736\n",
      "INFO:SupTrainer:Validate - Epoch: 3, Loss: 1.6736, Metrics: L1: 1.6736, L2: 1.6736\n",
      "INFO:SupTrainer:Train - Epoch: 4, Loss: 2.0926, Metrics: L1: 2.0926, L2: 2.0926\n",
      "INFO:SupTrainer:Validate - Epoch: 4, Loss: 2.0926, Metrics: L1: 2.0926, L2: 2.0926\n",
      "INFO:SupTrainer:Train - Epoch: 5, Loss: 1.3127, Metrics: L1: 1.3127, L2: 1.3127\n",
      "INFO:SupTrainer:Validate - Epoch: 5, Loss: 1.3127, Metrics: L1: 1.3127, L2: 1.3127\n",
      "INFO:SupTrainer:Train - Epoch: 1, Loss: 1.2019, Metrics: L1: 1.2019, L2: 1.2019\n",
      "INFO:SupTrainer:Validate - Epoch: 1, Loss: 1.2019, Metrics: L1: 1.2019, L2: 1.2019\n",
      "INFO:SupTrainer:Train - Epoch: 2, Loss: 1.2647, Metrics: L1: 1.2647, L2: 1.2647\n",
      "INFO:SupTrainer:Validate - Epoch: 2, Loss: 1.2647, Metrics: L1: 1.2647, L2: 1.2647\n",
      "INFO:SupTrainer:Train - Epoch: 3, Loss: 1.4404, Metrics: L1: 1.4404, L2: 1.4404\n",
      "INFO:SupTrainer:Validate - Epoch: 3, Loss: 1.4404, Metrics: L1: 1.4404, L2: 1.4404\n",
      "INFO:SupTrainer:Train - Epoch: 4, Loss: 1.2357, Metrics: L1: 1.2357, L2: 1.2357\n",
      "INFO:SupTrainer:Validate - Epoch: 4, Loss: 1.2357, Metrics: L1: 1.2357, L2: 1.2357\n",
      "INFO:SupTrainer:Train - Epoch: 5, Loss: 1.2724, Metrics: L1: 1.2724, L2: 1.2724\n",
      "INFO:SupTrainer:Validate - Epoch: 5, Loss: 1.2724, Metrics: L1: 1.2724, L2: 1.2724\n",
      "INFO:SupTrainer:Train - Epoch: 1, Loss: 1.2055, Metrics: L1: 1.2055, L2: 1.2055\n",
      "INFO:SupTrainer:Validate - Epoch: 1, Loss: 1.2055, Metrics: L1: 1.2055, L2: 1.2055\n",
      "INFO:SupTrainer:Train - Epoch: 2, Loss: 1.2275, Metrics: L1: 1.2275, L2: 1.2275\n",
      "INFO:SupTrainer:Validate - Epoch: 2, Loss: 1.2275, Metrics: L1: 1.2275, L2: 1.2275\n",
      "INFO:SupTrainer:Train - Epoch: 3, Loss: 1.3972, Metrics: L1: 1.3972, L2: 1.3972\n",
      "INFO:SupTrainer:Validate - Epoch: 3, Loss: 1.3972, Metrics: L1: 1.3972, L2: 1.3972\n",
      "INFO:SupTrainer:Train - Epoch: 4, Loss: 1.1681, Metrics: L1: 1.1681, L2: 1.1681\n",
      "INFO:SupTrainer:Validate - Epoch: 4, Loss: 1.1681, Metrics: L1: 1.1681, L2: 1.1681\n",
      "INFO:SupTrainer:Train - Epoch: 5, Loss: 1.1997, Metrics: L1: 1.1997, L2: 1.1997\n",
      "INFO:SupTrainer:Validate - Epoch: 5, Loss: 1.1997, Metrics: L1: 1.1997, L2: 1.1997\n",
      "INFO:SupTrainer:Train - Epoch: 1, Loss: 1.1512, Metrics: L1: 1.1512, L2: 1.1512\n",
      "INFO:SupTrainer:Validate - Epoch: 1, Loss: 1.1512, Metrics: L1: 1.1512, L2: 1.1512\n",
      "INFO:SupTrainer:Train - Epoch: 2, Loss: 1.2074, Metrics: L1: 1.2074, L2: 1.2074\n",
      "INFO:SupTrainer:Validate - Epoch: 2, Loss: 1.2074, Metrics: L1: 1.2074, L2: 1.2074\n",
      "INFO:SupTrainer:Train - Epoch: 3, Loss: 1.1455, Metrics: L1: 1.1455, L2: 1.1455\n",
      "INFO:SupTrainer:Validate - Epoch: 3, Loss: 1.1455, Metrics: L1: 1.1455, L2: 1.1455\n",
      "INFO:SupTrainer:Train - Epoch: 4, Loss: 1.4176, Metrics: L1: 1.4176, L2: 1.4176\n",
      "INFO:SupTrainer:Validate - Epoch: 4, Loss: 1.4176, Metrics: L1: 1.4176, L2: 1.4176\n",
      "INFO:SupTrainer:Train - Epoch: 5, Loss: 1.1847, Metrics: L1: 1.1847, L2: 1.1847\n",
      "INFO:SupTrainer:Validate - Epoch: 5, Loss: 1.1847, Metrics: L1: 1.1847, L2: 1.1847\n",
      "INFO:SupTrainer:Train - Epoch: 1, Loss: 1.2434, Metrics: L1: 1.2434, L2: 1.2434\n",
      "INFO:SupTrainer:Validate - Epoch: 1, Loss: 1.2434, Metrics: L1: 1.2434, L2: 1.2434\n",
      "INFO:SupTrainer:Train - Epoch: 2, Loss: 1.1576, Metrics: L1: 1.1576, L2: 1.1576\n",
      "INFO:SupTrainer:Validate - Epoch: 2, Loss: 1.1576, Metrics: L1: 1.1576, L2: 1.1576\n",
      "INFO:SupTrainer:Train - Epoch: 3, Loss: 1.2150, Metrics: L1: 1.2150, L2: 1.2150\n",
      "INFO:SupTrainer:Validate - Epoch: 3, Loss: 1.2150, Metrics: L1: 1.2150, L2: 1.2150\n",
      "INFO:SupTrainer:Train - Epoch: 4, Loss: 1.2391, Metrics: L1: 1.2391, L2: 1.2391\n",
      "INFO:SupTrainer:Validate - Epoch: 4, Loss: 1.2391, Metrics: L1: 1.2391, L2: 1.2391\n",
      "INFO:SupTrainer:Train - Epoch: 5, Loss: 1.1908, Metrics: L1: 1.1908, L2: 1.1908\n",
      "INFO:SupTrainer:Validate - Epoch: 5, Loss: 1.1908, Metrics: L1: 1.1908, L2: 1.1908\n",
      "INFO:SupTrainer:Train - Epoch: 1, Loss: 1.1942, Metrics: L1: 1.1942, L2: 1.1942\n",
      "INFO:SupTrainer:Validate - Epoch: 1, Loss: 1.1942, Metrics: L1: 1.1942, L2: 1.1942\n",
      "INFO:SupTrainer:Train - Epoch: 2, Loss: 1.9197, Metrics: L1: 1.9197, L2: 1.9197\n",
      "INFO:SupTrainer:Validate - Epoch: 2, Loss: 1.9197, Metrics: L1: 1.9197, L2: 1.9197\n",
      "INFO:SupTrainer:Train - Epoch: 3, Loss: 1.9580, Metrics: L1: 1.9580, L2: 1.9580\n",
      "INFO:SupTrainer:Validate - Epoch: 3, Loss: 1.9580, Metrics: L1: 1.9580, L2: 1.9580\n",
      "INFO:SupTrainer:Train - Epoch: 4, Loss: 1.7393, Metrics: L1: 1.7393, L2: 1.7393\n",
      "INFO:SupTrainer:Validate - Epoch: 4, Loss: 1.7393, Metrics: L1: 1.7393, L2: 1.7393\n",
      "INFO:SupTrainer:Train - Epoch: 5, Loss: 1.3170, Metrics: L1: 1.3170, L2: 1.3170\n",
      "INFO:SupTrainer:Validate - Epoch: 5, Loss: 1.3170, Metrics: L1: 1.3170, L2: 1.3170\n",
      "INFO:SupTrainer:Train - Epoch: 1, Loss: 1.2396, Metrics: L1: 1.2396, L2: 1.2396\n",
      "INFO:SupTrainer:Validate - Epoch: 1, Loss: 1.2396, Metrics: L1: 1.2396, L2: 1.2396\n",
      "INFO:SupTrainer:Train - Epoch: 2, Loss: 1.4405, Metrics: L1: 1.4405, L2: 1.4405\n",
      "INFO:SupTrainer:Validate - Epoch: 2, Loss: 1.4405, Metrics: L1: 1.4405, L2: 1.4405\n",
      "INFO:SupTrainer:Train - Epoch: 3, Loss: 1.2744, Metrics: L1: 1.2744, L2: 1.2744\n",
      "INFO:SupTrainer:Validate - Epoch: 3, Loss: 1.2744, Metrics: L1: 1.2744, L2: 1.2744\n",
      "INFO:SupTrainer:Train - Epoch: 4, Loss: 1.3403, Metrics: L1: 1.3403, L2: 1.3403\n",
      "INFO:SupTrainer:Validate - Epoch: 4, Loss: 1.3403, Metrics: L1: 1.3403, L2: 1.3403\n",
      "INFO:SupTrainer:Train - Epoch: 5, Loss: 1.2082, Metrics: L1: 1.2082, L2: 1.2082\n",
      "INFO:SupTrainer:Validate - Epoch: 5, Loss: 1.2082, Metrics: L1: 1.2082, L2: 1.2082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.37 s  486 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "trainer.train(ham_layer, loader, loader) # lightning + torch + adjoint (+ omp-num-threads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:SupTrainer:Train - Epoch: 1, Loss: 0.8819, Metrics: L1: 0.8819, L2: 0.8819\n",
      "INFO:SupTrainer:Validate - Epoch: 1, Loss: 0.8819, Metrics: L1: 0.8819, L2: 0.8819\n",
      "INFO:SupTrainer:Train - Epoch: 2, Loss: 0.8601, Metrics: L1: 0.8601, L2: 0.8601\n",
      "INFO:SupTrainer:Validate - Epoch: 2, Loss: 0.8601, Metrics: L1: 0.8601, L2: 0.8601\n",
      "INFO:SupTrainer:Train - Epoch: 3, Loss: 0.7551, Metrics: L1: 0.7551, L2: 0.7551\n",
      "INFO:SupTrainer:Validate - Epoch: 3, Loss: 0.7551, Metrics: L1: 0.7551, L2: 0.7551\n",
      "INFO:SupTrainer:Train - Epoch: 4, Loss: 0.7119, Metrics: L1: 0.7119, L2: 0.7119\n",
      "INFO:SupTrainer:Validate - Epoch: 4, Loss: 0.7119, Metrics: L1: 0.7119, L2: 0.7119\n",
      "INFO:SupTrainer:Train - Epoch: 5, Loss: 0.7337, Metrics: L1: 0.7337, L2: 0.7337\n",
      "INFO:SupTrainer:Validate - Epoch: 5, Loss: 0.7337, Metrics: L1: 0.7337, L2: 0.7337\n",
      "INFO:SupTrainer:Train - Epoch: 1, Loss: 0.7237, Metrics: L1: 0.7237, L2: 0.7237\n",
      "INFO:SupTrainer:Validate - Epoch: 1, Loss: 0.7237, Metrics: L1: 0.7237, L2: 0.7237\n",
      "INFO:SupTrainer:Train - Epoch: 2, Loss: 0.7057, Metrics: L1: 0.7057, L2: 0.7057\n",
      "INFO:SupTrainer:Validate - Epoch: 2, Loss: 0.7057, Metrics: L1: 0.7057, L2: 0.7057\n",
      "INFO:SupTrainer:Train - Epoch: 3, Loss: 0.7730, Metrics: L1: 0.7730, L2: 0.7730\n",
      "INFO:SupTrainer:Validate - Epoch: 3, Loss: 0.7730, Metrics: L1: 0.7730, L2: 0.7730\n",
      "INFO:SupTrainer:Train - Epoch: 4, Loss: 0.8221, Metrics: L1: 0.8221, L2: 0.8221\n",
      "INFO:SupTrainer:Validate - Epoch: 4, Loss: 0.8221, Metrics: L1: 0.8221, L2: 0.8221\n",
      "INFO:SupTrainer:Train - Epoch: 5, Loss: 0.9249, Metrics: L1: 0.9249, L2: 0.9249\n",
      "INFO:SupTrainer:Validate - Epoch: 5, Loss: 0.9249, Metrics: L1: 0.9249, L2: 0.9249\n",
      "INFO:SupTrainer:Train - Epoch: 1, Loss: 0.7299, Metrics: L1: 0.7299, L2: 0.7299\n",
      "INFO:SupTrainer:Validate - Epoch: 1, Loss: 0.7299, Metrics: L1: 0.7299, L2: 0.7299\n",
      "INFO:SupTrainer:Train - Epoch: 2, Loss: 0.9240, Metrics: L1: 0.9240, L2: 0.9240\n",
      "INFO:SupTrainer:Validate - Epoch: 2, Loss: 0.9240, Metrics: L1: 0.9240, L2: 0.9240\n",
      "INFO:SupTrainer:Train - Epoch: 3, Loss: 0.8592, Metrics: L1: 0.8592, L2: 0.8592\n",
      "INFO:SupTrainer:Validate - Epoch: 3, Loss: 0.8592, Metrics: L1: 0.8592, L2: 0.8592\n",
      "INFO:SupTrainer:Train - Epoch: 4, Loss: 0.8320, Metrics: L1: 0.8320, L2: 0.8320\n",
      "INFO:SupTrainer:Validate - Epoch: 4, Loss: 0.8320, Metrics: L1: 0.8320, L2: 0.8320\n",
      "INFO:SupTrainer:Train - Epoch: 5, Loss: 0.8480, Metrics: L1: 0.8480, L2: 0.8480\n",
      "INFO:SupTrainer:Validate - Epoch: 5, Loss: 0.8480, Metrics: L1: 0.8480, L2: 0.8480\n",
      "INFO:SupTrainer:Train - Epoch: 1, Loss: 0.8392, Metrics: L1: 0.8392, L2: 0.8392\n",
      "INFO:SupTrainer:Validate - Epoch: 1, Loss: 0.8392, Metrics: L1: 0.8392, L2: 0.8392\n",
      "INFO:SupTrainer:Train - Epoch: 2, Loss: 0.8106, Metrics: L1: 0.8106, L2: 0.8106\n",
      "INFO:SupTrainer:Validate - Epoch: 2, Loss: 0.8106, Metrics: L1: 0.8106, L2: 0.8106\n",
      "INFO:SupTrainer:Train - Epoch: 3, Loss: 0.8261, Metrics: L1: 0.8261, L2: 0.8261\n",
      "INFO:SupTrainer:Validate - Epoch: 3, Loss: 0.8261, Metrics: L1: 0.8261, L2: 0.8261\n",
      "INFO:SupTrainer:Train - Epoch: 4, Loss: 0.8148, Metrics: L1: 0.8148, L2: 0.8148\n",
      "INFO:SupTrainer:Validate - Epoch: 4, Loss: 0.8148, Metrics: L1: 0.8148, L2: 0.8148\n",
      "INFO:SupTrainer:Train - Epoch: 5, Loss: 0.6975, Metrics: L1: 0.6975, L2: 0.6975\n",
      "INFO:SupTrainer:Validate - Epoch: 5, Loss: 0.6975, Metrics: L1: 0.6975, L2: 0.6975\n",
      "INFO:SupTrainer:Train - Epoch: 1, Loss: 0.6865, Metrics: L1: 0.6865, L2: 0.6865\n",
      "INFO:SupTrainer:Validate - Epoch: 1, Loss: 0.6865, Metrics: L1: 0.6865, L2: 0.6865\n",
      "INFO:SupTrainer:Train - Epoch: 2, Loss: 0.7614, Metrics: L1: 0.7614, L2: 0.7614\n",
      "INFO:SupTrainer:Validate - Epoch: 2, Loss: 0.7614, Metrics: L1: 0.7614, L2: 0.7614\n",
      "INFO:SupTrainer:Train - Epoch: 3, Loss: 0.7247, Metrics: L1: 0.7247, L2: 0.7247\n",
      "INFO:SupTrainer:Validate - Epoch: 3, Loss: 0.7247, Metrics: L1: 0.7247, L2: 0.7247\n",
      "INFO:SupTrainer:Train - Epoch: 4, Loss: 0.7828, Metrics: L1: 0.7828, L2: 0.7828\n",
      "INFO:SupTrainer:Validate - Epoch: 4, Loss: 0.7828, Metrics: L1: 0.7828, L2: 0.7828\n",
      "INFO:SupTrainer:Train - Epoch: 5, Loss: 0.7783, Metrics: L1: 0.7783, L2: 0.7783\n",
      "INFO:SupTrainer:Validate - Epoch: 5, Loss: 0.7783, Metrics: L1: 0.7783, L2: 0.7783\n",
      "INFO:SupTrainer:Train - Epoch: 1, Loss: 0.7981, Metrics: L1: 0.7981, L2: 0.7981\n",
      "INFO:SupTrainer:Validate - Epoch: 1, Loss: 0.7981, Metrics: L1: 0.7981, L2: 0.7981\n",
      "INFO:SupTrainer:Train - Epoch: 2, Loss: 0.9063, Metrics: L1: 0.9063, L2: 0.9063\n",
      "INFO:SupTrainer:Validate - Epoch: 2, Loss: 0.9063, Metrics: L1: 0.9063, L2: 0.9063\n",
      "INFO:SupTrainer:Train - Epoch: 3, Loss: 0.8383, Metrics: L1: 0.8383, L2: 0.8383\n",
      "INFO:SupTrainer:Validate - Epoch: 3, Loss: 0.8383, Metrics: L1: 0.8383, L2: 0.8383\n",
      "INFO:SupTrainer:Train - Epoch: 4, Loss: 0.7869, Metrics: L1: 0.7869, L2: 0.7869\n",
      "INFO:SupTrainer:Validate - Epoch: 4, Loss: 0.7869, Metrics: L1: 0.7869, L2: 0.7869\n",
      "INFO:SupTrainer:Train - Epoch: 5, Loss: 0.9209, Metrics: L1: 0.9209, L2: 0.9209\n",
      "INFO:SupTrainer:Validate - Epoch: 5, Loss: 0.9209, Metrics: L1: 0.9209, L2: 0.9209\n",
      "INFO:SupTrainer:Train - Epoch: 1, Loss: 0.8053, Metrics: L1: 0.8053, L2: 0.8053\n",
      "INFO:SupTrainer:Validate - Epoch: 1, Loss: 0.8053, Metrics: L1: 0.8053, L2: 0.8053\n",
      "INFO:SupTrainer:Train - Epoch: 2, Loss: 0.7592, Metrics: L1: 0.7592, L2: 0.7592\n",
      "INFO:SupTrainer:Validate - Epoch: 2, Loss: 0.7592, Metrics: L1: 0.7592, L2: 0.7592\n",
      "INFO:SupTrainer:Train - Epoch: 3, Loss: 0.8393, Metrics: L1: 0.8393, L2: 0.8393\n",
      "INFO:SupTrainer:Validate - Epoch: 3, Loss: 0.8393, Metrics: L1: 0.8393, L2: 0.8393\n",
      "INFO:SupTrainer:Train - Epoch: 4, Loss: 0.8012, Metrics: L1: 0.8012, L2: 0.8012\n",
      "INFO:SupTrainer:Validate - Epoch: 4, Loss: 0.8012, Metrics: L1: 0.8012, L2: 0.8012\n",
      "INFO:SupTrainer:Train - Epoch: 5, Loss: 0.8332, Metrics: L1: 0.8332, L2: 0.8332\n",
      "INFO:SupTrainer:Validate - Epoch: 5, Loss: 0.8332, Metrics: L1: 0.8332, L2: 0.8332\n",
      "INFO:SupTrainer:Train - Epoch: 1, Loss: 0.8630, Metrics: L1: 0.8630, L2: 0.8630\n",
      "INFO:SupTrainer:Validate - Epoch: 1, Loss: 0.8630, Metrics: L1: 0.8630, L2: 0.8630\n",
      "INFO:SupTrainer:Train - Epoch: 2, Loss: 0.9132, Metrics: L1: 0.9132, L2: 0.9132\n",
      "INFO:SupTrainer:Validate - Epoch: 2, Loss: 0.9132, Metrics: L1: 0.9132, L2: 0.9132\n",
      "INFO:SupTrainer:Train - Epoch: 3, Loss: 0.7490, Metrics: L1: 0.7490, L2: 0.7490\n",
      "INFO:SupTrainer:Validate - Epoch: 3, Loss: 0.7490, Metrics: L1: 0.7490, L2: 0.7490\n",
      "INFO:SupTrainer:Train - Epoch: 4, Loss: 0.9175, Metrics: L1: 0.9175, L2: 0.9175\n",
      "INFO:SupTrainer:Validate - Epoch: 4, Loss: 0.9175, Metrics: L1: 0.9175, L2: 0.9175\n",
      "INFO:SupTrainer:Train - Epoch: 5, Loss: 0.8164, Metrics: L1: 0.8164, L2: 0.8164\n",
      "INFO:SupTrainer:Validate - Epoch: 5, Loss: 0.8164, Metrics: L1: 0.8164, L2: 0.8164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.7 s  210 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "trainer.train(ham_layer, loader, loader) # default + torch + backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "trainer.train(ham_layer, loader, loader) # default + torch + adjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -T lprof0 -u 1.0 -f trainer._train_step trainer.train(ham_layer, loader, loader)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class MeasurementType(Enum):\n",
    "    \"\"\"Measurement type for a measurement layer.\"\"\"\n",
    "\n",
    "    \"\"\"Expectation: return expected value of observable.\"\"\"\n",
    "    Expectation\n",
    "    \"\"\"Probabilities: return vector of probabilities.\"\"\"\n",
    "    Probabilities = \"probabilities\"\n",
    "    \"\"\"Samples: return measurement samples.\"\"\"\n",
    "    Samples = \"samples\"\n",
    "\n",
    "example = MeasurementType.Expectation\n",
    "print(type(example.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qml_mor.trainer import RegressionTrainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import Adam\n",
    "from datetime import datetime\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.1, amsgrad=True)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/check_{}'.format(timestamp))\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "training = RegressionTrainer(optimizer, loss_fn, writer)\n",
    "loss = training.train(model, loader, loader)\n",
    "print(loss)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"model_bestmre\"\n",
    "state = torch.load(path)\n",
    "model.load_state_dict(state)\n",
    "Ypred = model(X)\n",
    "print(Y)\n",
    "print(\"==============\")\n",
    "print(Ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/fashion_trainer_20230517_113403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pennylane as qml\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from qml_mor.models import IQPEReuploadSU2Parity\n",
    "from qml_mor.trainer import AdamTorch\n",
    "from qml_mor.datagen import DataGenCapacity\n",
    "\n",
    "num_samples=1\n",
    "num_qubits = 3\n",
    "num_reups = 1\n",
    "num_layers = 1\n",
    "sizex = num_qubits\n",
    "\n",
    "datagen = DataGenCapacity(sizex=sizex, num_samples=num_samples)\n",
    "\n",
    "omega = 0.0\n",
    "init_theta = torch.randn(num_reups, num_qubits, requires_grad=True)\n",
    "theta = torch.randn(\n",
    "    num_reups, num_layers, num_qubits - 1, 2, requires_grad=True\n",
    ")\n",
    "W = torch.randn(2**num_qubits, requires_grad=True)\n",
    "\n",
    "params = [init_theta, theta, W]\n",
    "\n",
    "model = IQPEReuploadSU2Parity(omega)\n",
    "dev = qml.device(\"default.qubit\", wires=num_qubits, shots=None)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def qnn_model(x, params):\n",
    "    return model.qfunction(x, params)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "num_epochs = 300\n",
    "opt = AdamTorch(params, loss_fn, num_epochs=num_epochs, amsgrad=True)\n",
    "\n",
    "N = 10\n",
    "batch_size=int(\"inf\")\n",
    "shuffle=True\n",
    "data = datagen.gen_data(N)\n",
    "X = data[\"X\"]\n",
    "Y = data[\"Y\"][0]\n",
    "dataset = TensorDataset(X, Y)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "opt_params = opt.optimize(qnn_model, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nx = X.size(0)\n",
    "y_pred = torch.stack([qnn_model(X[k], opt_params) for k in range(Nx)])\n",
    "mre = torch.mean(torch.abs((Y - y_pred) / y_pred))\n",
    "\n",
    "print(mre)\n",
    "print(loss_fn(y_pred, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "X = torch.randn(5, 3)\n",
    "Y = torch.randn(5)\n",
    "batch_size = 2\n",
    "\n",
    "dataset = TensorDataset(X, Y)\n",
    "shuffle = True\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "print(X)\n",
    "print(Y)\n",
    "print(\"===========\")\n",
    "for X_, Y_ in loader:\n",
    "    print(X_.size(0))\n",
    "    print(X_)\n",
    "    print(Y_)\n",
    "    print(\"***********\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qml_mor.datagen import DataGenRademacher, NormalPrior\n",
    "prior = NormalPrior(3, seed=0)\n",
    "radem = DataGenRademacher(prior, 2, 3, seed=None)\n",
    "data = radem.gen_data(4)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pennylane as qml\n",
    "num_qubits = 3\n",
    "W = torch.randn(2**num_qubits, requires_grad=False)\n",
    "from qulearn.qlayer import parity_hamiltonian\n",
    "H = parity_hamiltonian(num_qubits, W)\n",
    "print(W)\n",
    "print(\"======\")\n",
    "Z = qml.PauliZ(wires=0)\n",
    "print(H.coeffs.requires_grad)\n",
    "W_ = torch.nn.Parameter()\n",
    "print(W_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "from qml_mor.models import IQPEReuploadSU2Parity\n",
    "\n",
    "num_qubits  = 3\n",
    "num_reps    = 2\n",
    "num_layers  = 2\n",
    "omega       = 1.\n",
    "torch.manual_seed(0)\n",
    "x           = torch.randn(num_qubits, requires_grad=False)\n",
    "init_theta  = torch.randn(num_reps, num_qubits, requires_grad=True)\n",
    "theta       = torch.randn(num_reps, num_layers, num_qubits-1, 2, requires_grad=True)\n",
    "W           = torch.randn(2**num_qubits, requires_grad=True)\n",
    "\n",
    "params = [init_theta, theta, W]\n",
    "\n",
    "def iqpe_reupload_su2_parity(\n",
    "    x, init_theta, theta, W, omega: float = 0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Quantum function that calculates the expectation value\n",
    "    of the parity of Pauli Z operators.\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): Input tensor of shape (num_qubits,)\n",
    "        init_theta (Tensor): Initial rotation angles for each qubit,\n",
    "            of shape (reps, num_qubits)\n",
    "        theta (Tensor): Rotation angles for each layer and each qubit,\n",
    "            of shape (reps, num_layers, num_qubits-1, 2)\n",
    "        W (Tensor): Observable weights of shape (2^num_qubits,)\n",
    "        omega (float, optional): Exponential feature scaling factor. Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "        QFuncOutput: Expectation value of the parity of Pauli Z operators\n",
    "    \"\"\"\n",
    "\n",
    "    shape_init = init_theta.shape\n",
    "    shape = theta.shape\n",
    "    if len(shape_init) != 2:\n",
    "        raise ValueError(\"Initial theta must be a 2-dim tensor\")\n",
    "    if len(shape) != 4:\n",
    "        raise ValueError(\"Theta must be a 4-dim tensor\")\n",
    "\n",
    "    num_qubits = len(x)\n",
    "    reps = shape_init[0]\n",
    "    wires = range(num_qubits)\n",
    "\n",
    "    for layer in range(reps):\n",
    "        features = 2 ** (omega * layer) * x\n",
    "        initial_layer_weights = init_theta[layer]\n",
    "        weights = theta[layer]\n",
    "\n",
    "        qml.IQPEmbedding(features=features, wires=wires)\n",
    "        qml.SimplifiedTwoDesign(\n",
    "            initial_layer_weights=initial_layer_weights,\n",
    "            weights=weights,\n",
    "            wires=wires,\n",
    "        )\n",
    "\n",
    "def iqpe_reupload_su2_meas(\n",
    "    x, init_theta, theta, W, omega: float = 0.0\n",
    "):\n",
    "    iqpe_reupload_su2_parity(x, init_theta, theta, W, omega)\n",
    "    obs = parities(len(x))\n",
    "    H = qml.Hamiltonian(W, obs)\n",
    "\n",
    "    return qml.sample()\n",
    "\n",
    "#qnn_model = IQPEReuploadSU2Parity(params, omega=1.0)\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=num_qubits, shots=None)\n",
    "qnode = qml.QNode(iqpe_reupload_su2_meas, dev, interface=\"torch\")\n",
    "#drawer = qml.draw(qnode, expansion_strategy=\"device\")\n",
    "#print(drawer(x, init_theta, theta, W))\n",
    "probs = qnode(x, init_theta, theta, W, omega)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = probs\n",
    "bitstrings = [''.join(str(b.item()) for b in sample) for sample in samples]\n",
    "bitstring_counts = {bs: bitstrings.count(bs) for bs in set(bitstrings)}\n",
    "print(samples)\n",
    "print(bitstrings)\n",
    "print(bitstring_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qml_mor.models import IQPEReuploadSU2Parity\n",
    "\n",
    "model = IQPEReuploadSU2Parity()\n",
    "\n",
    "qnode = qml.QNode(model.probabilities, dev, interface=\"torch\")\n",
    "probs = qnode(x, params)\n",
    "print(probs)\n",
    "print(model.Hamiltonian(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = model.Hamiltonian(params)\n",
    "sum = 0.0\n",
    "b = \"110\"\n",
    "\n",
    "for idx, O in enumerate(H.ops):\n",
    "\n",
    "    if not isinstance(O.name, list):\n",
    "        if O.name == \"Identity\":\n",
    "            sum += H.coeffs[idx]\n",
    "        elif O.name == \"PauliZ\":\n",
    "            i = O.wires[0]\n",
    "            sign = (-1)**(int(b[-1-i]))\n",
    "            sum += sign*H.coeffs[idx]\n",
    "        else:\n",
    "            raise ValueError(\"All operators must be PauliZ or Identity.\")\n",
    "    else:\n",
    "        if not all(name==\"PauliZ\" for name in O.name):\n",
    "            raise ValueError(\"All operators must be PauliZ or Identity.\")\n",
    "        \n",
    "        sign = 1\n",
    "        for w in O.wires:\n",
    "            sign *= (-1)**(int(b[-1-w]))\n",
    "\n",
    "        sum += sign*H.coeffs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs)\n",
    "marginal = qml.math.marginal_prob(probs, axis=[0])\n",
    "print(marginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qml_mor.models import parities\n",
    "\n",
    "n = 3\n",
    "test = parities(n)\n",
    "H = qml.Hamiltonian(W, test)\n",
    "print(H)\n",
    "print(H.ops)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature layer\n",
    "def feature_layer(x):\n",
    "    num_qubits = len(x)\n",
    "    qml.IQPEmbedding(x, wires=range(num_qubits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variational layer\n",
    "def variational_layer(init_theta, theta, num_qubits):\n",
    "    qml.SimplifiedTwoDesign(initial_layer_weights=init_theta, weights=theta, wires=range(num_qubits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observable / output layer\n",
    "def sequence_generator(n):\n",
    "    if n == 0:\n",
    "        return [[]]\n",
    "    else:\n",
    "        sequences = []\n",
    "        for sequence in sequence_generator(n-1):\n",
    "            sequences.append(sequence + [n-1])\n",
    "            sequences.append(sequence)\n",
    "        return sequences\n",
    "    \n",
    "def parities(n):\n",
    "    \n",
    "    seq = sequence_generator(n)\n",
    "    ops = []\n",
    "    for par in seq:\n",
    "        if par:\n",
    "            tmp = qml.PauliZ(par[0])\n",
    "            if len(par) > 1:\n",
    "                for i in par[1:]:\n",
    "                    tmp = tmp @ qml.PauliZ(i)\n",
    "\n",
    "            ops.append(tmp)\n",
    "\n",
    "    ops.append(qml.Identity(0))\n",
    "\n",
    "    return ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pennylane.templates import IQPEmbedding, SimplifiedTwoDesign\n",
    "# QNN model\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def qnn_model(x, init_theta, theta, W, omega=0.):\n",
    "\n",
    "    num_qubits = len(x)\n",
    "    shape_init = init_theta.shape\n",
    "    reps       = 1\n",
    "    if len(shape_init) < 1:\n",
    "        init_theta = [init_theta]\n",
    "        shape_init = init_theta.shape\n",
    "    reps       = shape_init[0]\n",
    "\n",
    "    shape_theta = theta.shape\n",
    "    reps_       = 1\n",
    "    if len(shape_theta) < 3:\n",
    "        theta       = [theta]\n",
    "        shape_theta = theta.shape\n",
    "    reps_       = shape_theta[0]\n",
    "\n",
    "    assert reps == reps_\n",
    "    for l in range(reps):\n",
    "        qml.IQPEmbedding(features=2**(omega*l)*x, wires=range(num_qubits))\n",
    "        qml.SimplifiedTwoDesign(initial_layer_weights=init_theta[l], weights=theta[l], wires=range(num_qubits))\n",
    "\n",
    "    obs = parities(num_qubits)\n",
    "    H   = qml.Hamiltonian(W, obs)\n",
    "\n",
    "    return qml.expval(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane.templates import IQPEmbedding\n",
    "\n",
    "n_wires = 3\n",
    "n_layers = 2\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=n_wires)\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def iqpe_circuit(features):\n",
    "    for layer in range(n_layers):\n",
    "        # Apply the IQPEmbedding template in a loop\n",
    "        IQPEmbedding(features=features[layer], wires=range(n_wires))\n",
    "\n",
    "    return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_wires)]\n",
    "\n",
    "features = np.random.random((n_layers, n_wires))\n",
    "\n",
    "result1 = iqpe_circuit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qubits  = 3\n",
    "num_reps    = 0\n",
    "num_layers  = 0\n",
    "omega       = 1.\n",
    "x           = torch.randn(num_qubits, requires_grad=False)\n",
    "init_theta  = torch.randn(num_reps, num_qubits, requires_grad=True)\n",
    "theta       = torch.randn(num_reps, num_layers, num_qubits-1, 2, requires_grad=True)\n",
    "W           = torch.randn(2**num_qubits, requires_grad=True)\n",
    "\n",
    "result1 = qnn_model(x, init_theta, theta, W)\n",
    "result2 = qnn_model(x, init_theta, theta, W)\n",
    "\n",
    "ret = torch.stack([result1, result2])\n",
    "result1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import qmc\n",
    "def generate_samples_r(d, S):\n",
    "    sampler = qmc.LatinHypercube(d=d)\n",
    "    r_samples = sampler.random(n=S)\n",
    "    return r_samples\n",
    "r = generate_samples_r(10, 2)\n",
    "print(len(r[0]))\n",
    "\n",
    "tmp = set()\n",
    "rng = np.random.default_rng(seed=0)\n",
    "b1 = tuple(rng.integers(0, 2, size=4))\n",
    "b2 = tuple(rng.integers(0, 2, size=4))\n",
    "tmp.add(b1)\n",
    "tmp.add(b2)\n",
    "print(tmp)\n",
    "arr = np.array(list(tmp))\n",
    "print(arr)\n",
    "arr[0, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Original list of tensors\n",
    "tensor_list = [torch.tensor([1., 2., 3.], requires_grad=True),\n",
    "               torch.tensor([4., 5., 6.], requires_grad=False)]\n",
    "\n",
    "est_params = [\n",
    "                        t.detach().clone().requires_grad_(t.requires_grad)\n",
    "                        for t in tensor_list\n",
    "                    ]\n",
    "tensor_list *= 2\n",
    "print(tensor_list)\n",
    "print(est_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = (f\"Stopping early\\n\"\n",
    "       \"Loss not improving\")\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the quantum device\n",
    "dev = qml.device(\"default.qubit\", wires=2)\n",
    "\n",
    "# Define the custom model as a PennyLane QNode\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def custom_model(x, y):\n",
    "    # Apply your quantum circuit here, which uses input x and trainable parameters y\n",
    "    # As an example, we'll use a simple circuit with one rotation gate parameterized by y:\n",
    "    qml.RY(y[0], wires=0)\n",
    "    qml.RX(x, wires=1)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    return qml.expval(qml.PauliZ(1))\n",
    "\n",
    "# Define the dataset (x_data, y_data)\n",
    "x_data = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5], dtype=torch.float32)\n",
    "y_data = torch.tensor([0.5, 0.4, 0.3, 0.2, 0.1], dtype=torch.float32)\n",
    "\n",
    "# Initialize the trainable parameters\n",
    "params = torch.tensor([0.0], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Set the hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam([params], lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    predictions = torch.tensor([custom_model(x, params) for x in x_data], dtype=torch.float32)\n",
    "    loss = loss_fn(predictions, y_data)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "print(f\"Trained parameters: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qubits  = 3\n",
    "num_reps    = 3\n",
    "num_layers  = 0\n",
    "omega       = 1.\n",
    "x           = torch.randn(num_qubits, requires_grad=False)\n",
    "init_theta  = torch.randn(num_reps, num_qubits, requires_grad=True)\n",
    "theta       = torch.randn(num_reps, num_layers, num_qubits-1, 2, requires_grad=True)\n",
    "W           = torch.randn(2**num_qubits, requires_grad=True)\n",
    "def qnn_model(x, init_theta, theta, W, omega=0.):\n",
    "\n",
    "    num_qubits = len(x)\n",
    "    shape_init = init_theta.shape\n",
    "    reps       = 1\n",
    "    if len(shape_init) < 1:\n",
    "        init_theta = [init_theta]\n",
    "        shape_init = init_theta.shape\n",
    "    reps       = shape_init[0]\n",
    "\n",
    "    shape_theta = theta.shape\n",
    "    reps_       = 1\n",
    "    if len(shape_theta) < 3:\n",
    "        theta       = [theta]\n",
    "        shape_theta = theta.shape\n",
    "    reps_       = shape_theta[0]\n",
    "\n",
    "    assert reps == reps_\n",
    "    for l in range(reps):\n",
    "        qml.IQPEmbedding(features=2**(omega*l)*x, wires=range(num_qubits))\n",
    "        qml.SimplifiedTwoDesign(initial_layer_weights=init_theta[l], weights=theta[l], wires=range(num_qubits))\n",
    "\n",
    "    obs = parities(num_qubits)\n",
    "    H   = qml.Hamiltonian(W, obs)\n",
    "\n",
    "    return qml.expval(H)\n",
    "\n",
    "result = qnn_model(x, init_theta, theta, W)\n",
    "print(result.shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = parities(3)\n",
    "for x in tmp:\n",
    "    print(type(x))\n",
    "    print(issubclass(type(x), qml.operation.Observable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "num_qubits  = 3\n",
    "num_reps    = 3\n",
    "num_layers  = 0\n",
    "omega       = 1.\n",
    "x           = torch.randn(num_qubits, requires_grad=False)\n",
    "init_theta  = torch.randn(num_reps, num_qubits, requires_grad=True)\n",
    "theta       = torch.randn(num_reps, num_layers, num_qubits-1, 2, requires_grad=True)\n",
    "W           = torch.randn(2**num_qubits, requires_grad=True)\n",
    "\n",
    "print(qml.draw_mpl(qnn_model)(x, init_theta, theta, W, omega))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets for Capacity Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset(N, samples=10, seed=0):\n",
    "    sizex   = num_qubits\n",
    "    scale   = 2.\n",
    "    shift   = -1.\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    x       = scale*torch.rand(N, sizex, requires_grad=False) + shift\n",
    "    y       = scale*torch.rand(samples, N, requires_grad=False) + shift\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model specs\n",
    "num_qubits  = 3\n",
    "num_reps    = 2\n",
    "num_layers  = 2\n",
    "omega       = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial parameters\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "init_theta  = torch.randn(num_reps, num_qubits, requires_grad=True)\n",
    "theta       = torch.randn(num_reps, num_layers, num_qubits-1, 2, requires_grad=True)\n",
    "W           = torch.randn(2**num_qubits, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def square_loss(targets, predictions):\n",
    "    loss = 0\n",
    "    for t, p in zip(targets, predictions):\n",
    "        loss += (t - p) ** 2\n",
    "    loss = loss / len(targets)\n",
    "    return 0.5*loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capacity estimation parameters\n",
    "Nmin     = 1\n",
    "Nmax     = 5\n",
    "samples  = 10\n",
    "steps    = 300\n",
    "eps_stop = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {}\n",
    "for N in range(Nmin, Nmax):\n",
    "    x, y = gen_dataset(N, samples)\n",
    "    \n",
    "    mre_sample = []\n",
    "    for s in range(samples):\n",
    "        print('===================================')\n",
    "        def cost(init_theta, theta, W):\n",
    "            pred = [qnn_model(x[k], init_theta, theta, W) for k in range(N)]\n",
    "            loss = square_loss(y[s], pred)\n",
    "            return loss\n",
    "        \n",
    "        # optimize\n",
    "        opt = torch.optim.Adam([init_theta, theta, W], lr=0.1, amsgrad=True)\n",
    "        for n in range(steps):\n",
    "            opt.zero_grad()\n",
    "            loss = cost(init_theta, theta, W)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            if n%10 == 9 or n == steps - 1:\n",
    "                print(f'{n+1}: {loss}')\n",
    "\n",
    "            if loss <= eps_stop:\n",
    "                break\n",
    "\n",
    "        # compute prediction errors\n",
    "        y_pred = torch.tensor([qnn_model(x[k], init_theta, theta, W) for k in range(N)], requires_grad=False)\n",
    "        mre    = torch.mean(torch.abs((y[s]-y_pred)/y_pred))\n",
    "        mre_sample.append(mre)\n",
    "\n",
    "    mre_N = torch.mean(torch.tensor(mre_sample))\n",
    "    summary[f'N = {N}'] = mre_N.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count, eps in enumerate(summary.values()):\n",
    "    m = int(np.log2(1./eps.item()))\n",
    "    C = (count+1)*m\n",
    "    print(C)\n",
    "print(torch.numel(init_theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [-0.5, 13.1, 2]\n",
    "max(C)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.tensor([qnn_model(x[k], init_theta, theta, W) for k in range(N)], requires_grad=False)\n",
    "mre    = torch.mean(torch.abs((y[s]-y_pred)/y_pred))\n",
    "print(mre.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutoff precision converted to bits of precision\n",
    "cutoff = np.sqrt(eps_stop)\n",
    "m      = np.log2(1./cutoff)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)\n",
    "print(init_theta)\n",
    "print(theta)\n",
    "print(W)\n",
    "print(y[s])\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QML-MOR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46e7eb4246c13550057ff7606884a772512028b1851bd0c4d2a0ba2e0c036465"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
